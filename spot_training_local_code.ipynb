{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5db8829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'551329315830.dkr.ecr.us-east-1.amazonaws.com/pytorch-tft-container-test:latest'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client=boto3.client('sts')\n",
    "account=client.get_caller_identity()['Account']\n",
    "\n",
    "my_session=boto3.session.Session()\n",
    "region=my_session.region_name\n",
    "\n",
    "algorithm_name=\"pytorch-tft-container-test\"\n",
    "ecr_image='{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "\n",
    "ecr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da46e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\r\n",
      "Configure a credential helper to remove this warning. See\r\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\r\n",
      "\r\n",
      "Login Succeeded\r\n"
     ]
    }
   ],
   "source": [
    "! aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc04265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker version: 2.103.0\n",
      "Checkpointing Path: s3://sagemaker-us-east-1-551329315830/checkpoints/checkpoint-50760748\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import uuid\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "print('SageMaker version: ' + sagemaker.__version__)\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-cnn-cifar10'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_path = 's3://{}/checkpoints/checkpoint-{}'.format(bucket, checkpoint_suffix)\n",
    "\n",
    "print('Checkpointing Path: {}'.format(checkpoint_s3_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018412a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointing directory timeseries_data exists\n",
      "saved raw data to timeseries_data/stallion_data.parquet\n",
      "Checkpointing directory timeseries_data exists\n",
      "saved metadata to timeseries_data/stallion_metadata.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-551329315830/data/timeseries_data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_timeseries import download_process_and_return_raw_data, save_local_and_upload_s3, metadata_json_upload_s3\n",
    "import sagemaker\n",
    "import uuid\n",
    "\n",
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "special_days = [\n",
    "        \"easter_day\",\n",
    "        \"good_friday\",\n",
    "        \"new_year\",\n",
    "        \"christmas\",\n",
    "        \"labor_day\",\n",
    "        \"independence_day\",\n",
    "        \"revolution_day_memorial\",\n",
    "        \"regional_games\",\n",
    "        \"fifa_u_17_world_cup\",\n",
    "        \"football_gold_cup\",\n",
    "        \"beer_capital\",\n",
    "        \"music_fest\",\n",
    "    ]\n",
    "\n",
    "training_metadata = {}\n",
    "training_metadata['time_idx'] = \"time_idx\"\n",
    "training_metadata['target'] = \"volume\"\n",
    "training_metadata['group_ids'] = [\"agency\", \"sku\"]\n",
    "training_metadata['min_encoder_length'] = max_encoder_length // 2      # keep encoder length long (as it is in the validation set)\n",
    "training_metadata['max_encoder_length'] = max_encoder_length\n",
    "training_metadata['min_prediction_length'] = 1      \n",
    "training_metadata['max_prediction_length'] = max_prediction_length\n",
    "training_metadata['static_categoricals'] = [\"agency\", \"sku\"]\n",
    "training_metadata['static_reals'] = [\"avg_population_2017\", \"avg_yearly_household_income_2017\"]\n",
    "training_metadata['time_varying_known_categoricals'] = [\"special_days\", \"month\"]\n",
    "training_metadata['variable_groups'] = {\"special_days\": special_days}\n",
    "training_metadata['time_varying_known_reals'] = [\"time_idx\", \"price_regular\", \"discount_in_percent\"]\n",
    "training_metadata['time_varying_unknown_categoricals'] = []\n",
    "training_metadata['time_varying_unknown_reals'] = [\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ]\n",
    "training_metadata['target_normalizer'] = {\n",
    "                            \"normalized_groups\": [\"agency\", \"sku\"],\n",
    "                            \"normalization_transformation\": 'softplus'\n",
    "                        }\n",
    "training_metadata['add_relative_time_idx'] = True\n",
    "training_metadata['add_target_scales'] = True\n",
    "training_metadata['add_encoder_length'] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# obtain & upload training data\n",
    "training_data = download_process_and_return_raw_data()\n",
    "inputs = save_local_and_upload_s3(training_data, sagemaker_session, bucket, data_filename=\"stallion_data\")\n",
    "\n",
    "# upload metadata\n",
    "training_metadata['training_cutoff'] = int(training_data[\"time_idx\"].max() - max_prediction_length)\n",
    "metadata_json_upload_s3(training_metadata, sagemaker_session, bucket, metadata_filename=\"stallion_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b357b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot_instances = False\n",
    "max_run=600      # in seconds, after this, job will be terminated\n",
    "max_wait = 10 * max_run if use_spot_instances else None\n",
    "local_image_name = 'pytorch-tft-container-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684b0a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-21 17:28:46 Starting - Starting the training job...ProfilerReport-1661102926: InProgress\n",
      "...\n",
      "2022-08-21 17:29:29 Starting - Preparing the instances for training......\n",
      "2022-08-21 17:30:37 Downloading - Downloading input data...\n",
      "2022-08-21 17:31:10 Training - Downloading the training image..................\n",
      "2022-08-21 17:34:14 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-08-21 17:34:16,962 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-08-21 17:34:16,986 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-08-21 17:34:16,994 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-08-21 17:34:17,420 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting pytorch-forecasting\n",
      "  Downloading pytorch_forecasting-0.9.0-py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34mCollecting fastparquet\n",
      "  Downloading fastparquet-0.8.0.tar.gz (400 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (4.0.0)\u001b[0m\n",
      "\u001b[34mCollecting optuna<3.0.0,>=2.3.0\n",
      "  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\u001b[0m\n",
      "\u001b[34mCollecting statsmodels\n",
      "  Downloading statsmodels-0.12.2-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn<0.25,>=0.23 in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<2.0.0,>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (3.3.4)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning<2.0.0,>=1.2.4\n",
      "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mCollecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting colorlog\n",
      "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting cliff\n",
      "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.4.40-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas<2.0.0,>=1.1.0->pytorch-forecasting->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas<2.0.0,>=1.1.0->pytorch-forecasting->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas<2.0.0,>=1.1.0->pytorch-forecasting->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mCollecting setuptools==59.5.0\u001b[0m\n",
      "\u001b[34m  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics>=0.4.1\n",
      "  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn<0.25,>=0.23->pytorch-forecasting->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn<0.25,>=0.23->pytorch-forecasting->-r requirements.txt (line 1)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (3.17.1)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (0.35.1)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch<2.0,>=1.7->pytorch-forecasting->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.5.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (191 kB)\u001b[0m\n",
      "\u001b[34mCollecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mCollecting idna-ssl>=1.0\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (21.2.0)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mCollecting Mako\n",
      "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34mCollecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34mCollecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mCollecting autopage>=0.4.0\n",
      "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34mCollecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-2.5.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (0.2.5)\u001b[0m\n",
      "\u001b[34mCollecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.6/site-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pytorch-forecasting->-r requirements.txt (line 1)) (8.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pytorch-forecasting->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pytorch-forecasting->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mCollecting patsy>=0.5\n",
      "  Downloading patsy-0.5.2-py2.py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fastparquet, idna-ssl, pyperclip\n",
      "  Building wheel for fastparquet (setup.py): started\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m  Building wheel for fastparquet (setup.py): finished with status 'done'\n",
      "  Created wheel for fastparquet: filename=fastparquet-0.8.0-cp36-cp36m-linux_x86_64.whl size=1256928 sha256=0e0768739f0a286dcf2ecc7c43997999df35df2cbda833b4c2951dfe40927ca2\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/6a/4f/0fd8e8bcbc4b5b751186e363b5b03975d8643eee2975eed2ca\n",
      "  Building wheel for idna-ssl (setup.py): started\n",
      "  Building wheel for idna-ssl (setup.py): finished with status 'done'\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=5e508e1a302ca9569c1c38dca81846c8bf1bd55e80068350582788ea5cbd1282\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "  Building wheel for pyperclip (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11106 sha256=818c93fd502a263d67914cb0d6766779764aad69af87a752aef846113a72894b\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/95/e30a7f0b44cb90642de3469f211a3218f93f871789b4f4b46c\u001b[0m\n",
      "\u001b[34mSuccessfully built fastparquet idna-ssl pyperclip\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyasn1-modules, oauthlib, multidict, frozenlist, cachetools, yarl, requests-oauthlib, pyperclip, pbr, importlib-metadata, idna-ssl, google-auth, charset-normalizer, asynctest, async-timeout, aiosignal, tensorboard-plugin-wit, tensorboard-data-server, stevedore, sqlalchemy, setuptools, pyDeprecate, PrettyTable, markdown, Mako, importlib-resources, grpcio, google-auth-oauthlib, cmd2, autopage, aiohttp, absl-py, torchmetrics, tensorboard, patsy, colorlog, cmaes, cliff, alembic, statsmodels, pytorch-lightning, optuna, cramjam, pytorch-forecasting, fastparquet\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.0.1\n",
      "    Uninstalling importlib-metadata-4.0.1:\n",
      "      Successfully uninstalled importlib-metadata-4.0.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 49.6.0.post20210108\n",
      "    Uninstalling setuptools-49.6.0.post20210108:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled setuptools-49.6.0.post20210108\u001b[0m\n",
      "\u001b[34mSuccessfully installed Mako-1.1.6 PrettyTable-2.5.0 absl-py-1.2.0 aiohttp-3.8.1 aiosignal-1.2.0 alembic-1.7.7 async-timeout-4.0.2 asynctest-0.13.0 autopage-0.5.1 cachetools-4.2.4 charset-normalizer-2.1.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.6.0 cramjam-2.5.0 fastparquet-0.8.0 frozenlist-1.2.0 google-auth-2.11.0 google-auth-oauthlib-0.4.6 grpcio-1.47.0 idna-ssl-1.1.0 importlib-metadata-4.8.3 importlib-resources-5.4.0 markdown-3.3.7 multidict-5.2.0 oauthlib-3.2.0 optuna-2.10.1 patsy-0.5.2 pbr-5.10.0 pyDeprecate-0.3.1 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.9.0 pytorch-lightning-1.5.10 requests-oauthlib-1.3.1 setuptools-59.5.0 sqlalchemy-1.4.40 statsmodels-0.12.2 stevedore-3.5.0 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.8.2 yarl-1.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-08-21 17:35:00,022 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"data-filename\": \"stallion_data.parquet\",\n",
      "        \"epochs\": 5,\n",
      "        \"metadata-filename\": \"stallion_metadata.json\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tft-pytorch-spot-1-2022-08-21-17-28-46-598\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-08-21-17-28-46-598/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"TFT\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"TFT.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"data-filename\":\"stallion_data.parquet\",\"epochs\":5,\"metadata-filename\":\"stallion_metadata.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=TFT.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=TFT\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-08-21-17-28-46-598/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"data-filename\":\"stallion_data.parquet\",\"epochs\":5,\"metadata-filename\":\"stallion_metadata.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tft-pytorch-spot-1-2022-08-21-17-28-46-598\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-08-21-17-28-46-598/source/sourcedir.tar.gz\",\"module_name\":\"TFT\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"TFT.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--data-filename\",\"stallion_data.parquet\",\"--epochs\",\"5\",\"--metadata-filename\",\"stallion_metadata.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATA-FILENAME=stallion_data.parquet\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_METADATA-FILENAME=stallion_metadata.json\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 TFT.py --data-filename stallion_data.parquet --epochs 5 --metadata-filename stallion_metadata.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mCheckpointing directory /opt/ml/checkpoints exists\u001b[0m\n",
      "\u001b[34mDevice Type: cuda\u001b[0m\n",
      "\u001b[34mLoad Time Series dataset from S3\u001b[0m\n",
      "\u001b[34mcreating dataloader\u001b[0m\n",
      "\u001b[34mget GPU information\u001b[0m\n",
      "\u001b[34mGPU count: 1\u001b[0m\n",
      "\u001b[34mcreate model trainer\u001b[0m\n",
      "\u001b[34mcreate model from dataset\u001b[0m\n",
      "\u001b[34mNumber of parameters in network: 29.7k\u001b[0m\n",
      "\u001b[34mtraining model\u001b[0m\n",
      "\u001b[34m#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]#015Validation sanity check: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]#015                                                                      #015#015Training: 0it [00:00, ?it/s]#015Training:   0%|          | 0/31 [00:00<?, ?it/s]#015Epoch 0:   0%|          | 0/31 [00:00<?, ?it/s] #015Epoch 0:   3%|▎         | 1/31 [00:01<00:31,  1.07s/it]#015Epoch 0:   3%|▎         | 1/31 [00:01<00:31,  1.07s/it, loss=116, v_num=0, train_loss_step=116.0]#015Epoch 0:   6%|▋         | 2/31 [00:01<00:20,  1.39it/s, loss=116, v_num=0, train_loss_step=116.0]#015Epoch 0:   6%|▋         | 2/31 [00:01<00:20,  1.39it/s, loss=123, v_num=0, train_loss_step=130.0]#015Epoch 0:  10%|▉         | 3/31 [00:01<00:16,  1.67it/s, loss=123, v_num=0, train_loss_step=130.0]#015Epoch 0:  10%|▉         | 3/31 [00:01<00:16,  1.67it/s, loss=137, v_num=0, train_loss_step=165.0]#015Epoch 0:  13%|█▎        | 4/31 [00:02<00:14,  1.84it/s, loss=137, v_num=0, train_loss_step=165.0]#015Epoch 0:  13%|█▎        | 4/31 [00:02<00:14,  1.84it/s, loss=135, v_num=0, train_loss_step=129.0]#015Epoch 0:  16%|█▌        | 5/31 [00:02<00:13,  1.96it/s, loss=135, v_num=0, train_loss_step=129.0]#015Epoch 0:  16%|█▌        | 5/31 [00:02<00:13,  1.96it/s, loss=136, v_num=0, train_loss_step=141.0]#015Epoch 0:  19%|█▉        | 6/31 [00:02<00:12,  2.03it/s, loss=136, v_num=0, train_loss_step=141.0]#015Epoch 0:  19%|█▉        | 6/31 [00:02<00:12,  2.03it/s, loss=145, v_num=0, train_loss_step=186.0]#015Epoch 0:  23%|██▎       | 7/31 [00:03<00:11,  2.10it/s, loss=145, v_num=0, train_loss_step=186.0]#015Epoch 0:  23%|██▎       | 7/31 [00:03<00:11,  2.10it/s, loss=147, v_num=0, train_loss_step=160.0]#015Epoch 0:  26%|██▌       | 8/31 [00:03<00:10,  2.16it/s, loss=147, v_num=0, train_loss_step=160.0]#015Epoch 0:  26%|██▌       | 8/31 [00:03<00:10,  2.16it/s, loss=142, v_num=0, train_loss_step=111.0]#015Epoch 0:  29%|██▉       | 9/31 [00:04<00:09,  2.20it/s, loss=142, v_num=0, train_loss_step=111.0]#015Epoch 0:  29%|██▉       | 9/31 [00:04<00:09,  2.20it/s, loss=146, v_num=0, train_loss_step=173.0]#015Epoch 0:  32%|███▏      | 10/31 [00:04<00:09,  2.24it/s, loss=146, v_num=0, train_loss_step=173.0]#015Epoch 0:  32%|███▏      | 10/31 [00:04<00:09,  2.24it/s, loss=145, v_num=0, train_loss_step=135.0]#015Epoch 0:  35%|███▌      | 11/31 [00:05<00:09,  2.14it/s, loss=145, v_num=0, train_loss_step=135.0]#015Epoch 0:  35%|███▌      | 11/31 [00:05<00:09,  2.14it/s, loss=142, v_num=0, train_loss_step=119.0]#015Epoch 0:  39%|███▊      | 12/31 [00:05<00:08,  2.17it/s, loss=142, v_num=0, train_loss_step=119.0]#015Epoch 0:  39%|███▊      | 12/31 [00:05<00:08,  2.16it/s, loss=140, v_num=0, train_loss_step=113.0]#015Epoch 0:  42%|████▏     | 13/31 [00:05<00:08,  2.20it/s, loss=140, v_num=0, train_loss_step=113.0]#015Epoch 0:  42%|████▏     | 13/31 [00:05<00:08,  2.20it/s, loss=142, v_num=0, train_loss_step=164.0]#015Epoch 0:  45%|████▌     | 14/31 [00:06<00:07,  2.23it/s, loss=142, v_num=0, train_loss_step=164.0]#015Epoch 0:  45%|████▌     | 14/31 [00:06<00:07,  2.23it/s, loss=139, v_num=0, train_loss_step=105.0]#015Epoch 0:  48%|████▊     | 15/31 [00:06<00:07,  2.25it/s, loss=139, v_num=0, train_loss_step=105.0]#015Epoch 0:  48%|████▊     | 15/31 [00:06<00:07,  2.25it/s, loss=137, v_num=0, train_loss_step=110.0]#015Epoch 0:  52%|█████▏    | 16/31 [00:07<00:06,  2.27it/s, loss=137, v_num=0, train_loss_step=110.0]#015Epoch 0:  52%|█████▏    | 16/31 [00:07<00:06,  2.27it/s, loss=135, v_num=0, train_loss_step=100.0]#015Epoch 0:  55%|█████▍    | 17/31 [00:07<00:06,  2.28it/s, loss=135, v_num=0, train_loss_step=100.0]#015Epoch 0:  55%|█████▍    | 17/31 [00:07<00:06,  2.28it/s, loss=134, v_num=0, train_loss_step=125.0]#015Epoch 0:  58%|█████▊    | 18/31 [00:07<00:05,  2.28it/s, loss=134, v_num=0, train_loss_step=125.0]#015Epoch 0:  58%|█████▊    | 18/31 [00:07<00:05,  2.28it/s, loss=132, v_num=0, train_loss_step=95.40]#015Epoch 0:  61%|██████▏   | 19/31 [00:08<00:05,  2.28it/s, loss=132, v_num=0, train_loss_step=95.40]#015Epoch 0:  61%|██████▏   | 19/31 [00:08<00:05,  2.28it/s, loss=131, v_num=0, train_loss_step=118.0]#015Epoch 0:  65%|██████▍   | 20/31 [00:08<00:04,  2.29it/s, loss=131, v_num=0, train_loss_step=118.0]#015Epoch 0:  65%|██████▍   | 20/31 [00:08<00:04,  2.29it/s, loss=131, v_num=0, train_loss_step=120.0]#015Epoch 0:  68%|██████▊   | 21/31 [00:09<00:04,  2.24it/s, loss=131, v_num=0, train_loss_step=120.0]#015Epoch 0:  68%|██████▊   | 21/31 [00:09<00:04,  2.24it/s, loss=129, v_num=0, train_loss_step=79.00]#015Epoch 0:  71%|███████   | 22/31 [00:09<00:03,  2.26it/s, loss=129, v_num=0, train_loss_step=79.00]#015Epoch 0:  71%|███████   | 22/31 [00:09<00:03,  2.26it/s, loss=127, v_num=0, train_loss_step=84.40]#015Epoch 0:  74%|███████▍  | 23/31 [00:10<00:03,  2.27it/s, loss=127, v_num=0, train_loss_step=84.40]#015Epoch 0:  74%|███████▍  | 23/31 [00:10<00:03,  2.27it/s, loss=123, v_num=0, train_loss_step=97.70]#015Epoch 0:  77%|███████▋  | 24/31 [00:10<00:03,  2.28it/s, loss=123, v_num=0, train_loss_step=97.70]#015Epoch 0:  77%|███████▋  | 24/31 [00:10<00:03,  2.28it/s, loss=121, v_num=0, train_loss_step=83.00]#015Epoch 0:  81%|████████  | 25/31 [00:10<00:02,  2.29it/s, loss=121, v_num=0, train_loss_step=83.00]#015Epoch 0:  81%|████████  | 25/31 [00:10<00:02,  2.29it/s, loss=119, v_num=0, train_loss_step=104.0]#015Epoch 0:  84%|████████▍ | 26/31 [00:11<00:02,  2.30it/s, loss=119, v_num=0, train_loss_step=104.0]#015Epoch 0:  84%|████████▍ | 26/31 [00:11<00:02,  2.30it/s, loss=114, v_num=0, train_loss_step=90.50]#015Epoch 0:  87%|████████▋ | 27/31 [00:11<00:01,  2.31it/s, loss=114, v_num=0, train_loss_step=90.50]#015Epoch 0:  87%|████████▋ | 27/31 [00:11<00:01,  2.31it/s, loss=110, v_num=0, train_loss_step=77.40]#015Epoch 0:  90%|█████████ | 28/31 [00:12<00:01,  2.31it/s, loss=110, v_num=0, train_loss_step=77.40]#015Epoch 0:  90%|█████████ | 28/31 [00:12<00:01,  2.31it/s, loss=109, v_num=0, train_loss_step=80.90]#015Epoch 0:  94%|█████████▎| 29/31 [00:12<00:00,  2.32it/s, loss=109, v_num=0, train_loss_step=80.90]#015Epoch 0:  94%|█████████▎| 29/31 [00:12<00:00,  2.32it/s, loss=105, v_num=0, train_loss_step=107.0]#015Epoch 0:  97%|█████████▋| 30/31 [00:12<00:00,  2.32it/s, loss=105, v_num=0, train_loss_step=107.0]#015Epoch 0:  97%|█████████▋| 30/31 [00:12<00:00,  2.32it/s, loss=104, v_num=0, train_loss_step=101.0]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]#033[A#015Epoch 0: 100%|██████████| 31/31 [00:14<00:00,  2.14it/s, loss=104, v_num=0, train_loss_step=101.0, val_loss=125.0]\u001b[0m\n",
      "\u001b[34m#015                                                         #033[A#015Epoch 0: 100%|██████████| 31/31 [00:15<00:00,  2.02it/s, loss=104, v_num=0, train_loss_step=101.0, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 0:   0%|          | 0/31 [00:00<?, ?it/s, loss=104, v_num=0, train_loss_step=101.0, val_loss=125.0, train_loss_epoch=117.0]         #015Epoch 1:   0%|          | 0/31 [00:00<?, ?it/s, loss=104, v_num=0, train_loss_step=101.0, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, loss=104, v_num=0, train_loss_step=101.0, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, loss=101, v_num=0, train_loss_step=60.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:   6%|▋         | 2/31 [00:01<00:17,  1.65it/s, loss=101, v_num=0, train_loss_step=60.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:   6%|▋         | 2/31 [00:01<00:17,  1.65it/s, loss=98.2, v_num=0, train_loss_step=62.90, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  10%|▉         | 3/31 [00:01<00:15,  1.87it/s, loss=98.2, v_num=0, train_loss_step=62.90, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  10%|▉         | 3/31 [00:01<00:15,  1.86it/s, loss=94.7, v_num=0, train_loss_step=92.30, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  13%|█▎        | 4/31 [00:01<00:13,  2.01it/s, loss=94.7, v_num=0, train_loss_step=92.30, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  13%|█▎        | 4/31 [00:01<00:13,  2.00it/s, loss=93.1, v_num=0, train_loss_step=73.80, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  16%|█▌        | 5/31 [00:02<00:12,  2.08it/s, loss=93.1, v_num=0, train_loss_step=73.80, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  16%|█▌        | 5/31 [00:02<00:12,  2.08it/s, loss=92, v_num=0, train_loss_step=88.20, val_loss=125.0, train_loss_epoch=117.0]  #015Epoch 1:  19%|█▉        | 6/31 [00:02<00:11,  2.12it/s, loss=92, v_num=0, train_loss_step=88.20, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  19%|█▉        | 6/31 [00:02<00:11,  2.12it/s, loss=91.7, v_num=0, train_loss_step=93.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  23%|██▎       | 7/31 [00:03<00:11,  2.18it/s, loss=91.7, v_num=0, train_loss_step=93.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  23%|██▎       | 7/31 [00:03<00:11,  2.18it/s, loss=90.8, v_num=0, train_loss_step=107.0, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  26%|██▌       | 8/31 [00:03<00:10,  2.21it/s, loss=90.8, v_num=0, train_loss_step=107.0, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  26%|██▌       | 8/31 [00:03<00:10,  2.21it/s, loss=90.7, v_num=0, train_loss_step=93.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  29%|██▉       | 9/31 [00:04<00:09,  2.24it/s, loss=90.7, v_num=0, train_loss_step=93.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  29%|██▉       | 9/31 [00:04<00:09,  2.24it/s, loss=88.7, v_num=0, train_loss_step=78.70, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  32%|███▏      | 10/31 [00:04<00:09,  2.27it/s, loss=88.7, v_num=0, train_loss_step=78.70, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  32%|███▏      | 10/31 [00:04<00:09,  2.27it/s, loss=86.8, v_num=0, train_loss_step=81.30, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  35%|███▌      | 11/31 [00:05<00:09,  2.17it/s, loss=86.8, v_num=0, train_loss_step=81.30, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  35%|███▌      | 11/31 [00:05<00:09,  2.17it/s, loss=87.8, v_num=0, train_loss_step=100.0, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  39%|███▊      | 12/31 [00:05<00:08,  2.18it/s, loss=87.8, v_num=0, train_loss_step=100.0, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  39%|███▊      | 12/31 [00:05<00:08,  2.18it/s, loss=86.9, v_num=0, train_loss_step=65.20, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  42%|████▏     | 13/31 [00:05<00:08,  2.21it/s, loss=86.9, v_num=0, train_loss_step=65.20, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  42%|████▏     | 13/31 [00:05<00:08,  2.21it/s, loss=85.6, v_num=0, train_loss_step=73.30, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  45%|████▌     | 14/31 [00:06<00:07,  2.24it/s, loss=85.6, v_num=0, train_loss_step=73.30, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  45%|████▌     | 14/31 [00:06<00:07,  2.24it/s, loss=85.8, v_num=0, train_loss_step=86.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  48%|████▊     | 15/31 [00:06<00:07,  2.26it/s, loss=85.8, v_num=0, train_loss_step=86.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  48%|████▊     | 15/31 [00:06<00:07,  2.26it/s, loss=84.4, v_num=0, train_loss_step=76.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  52%|█████▏    | 16/31 [00:07<00:06,  2.28it/s, loss=84.4, v_num=0, train_loss_step=76.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  52%|█████▏    | 16/31 [00:07<00:06,  2.28it/s, loss=84.3, v_num=0, train_loss_step=88.20, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  55%|█████▍    | 17/31 [00:07<00:06,  2.29it/s, loss=84.3, v_num=0, train_loss_step=88.20, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  55%|█████▍    | 17/31 [00:07<00:06,  2.29it/s, loss=84, v_num=0, train_loss_step=71.90, val_loss=125.0, train_loss_epoch=117.0]  #015Epoch 1:  58%|█████▊    | 18/31 [00:07<00:05,  2.30it/s, loss=84, v_num=0, train_loss_step=71.90, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  58%|█████▊    | 18/31 [00:07<00:05,  2.30it/s, loss=85, v_num=0, train_loss_step=100.0, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  61%|██████▏   | 19/31 [00:08<00:05,  2.31it/s, loss=85, v_num=0, train_loss_step=100.0, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  61%|██████▏   | 19/31 [00:08<00:05,  2.31it/s, loss=83, v_num=0, train_loss_step=66.00, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  65%|██████▍   | 20/31 [00:08<00:04,  2.32it/s, loss=83, v_num=0, train_loss_step=66.00, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  65%|██████▍   | 20/31 [00:08<00:04,  2.32it/s, loss=81.1, v_num=0, train_loss_step=62.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  68%|██████▊   | 21/31 [00:09<00:04,  2.26it/s, loss=81.1, v_num=0, train_loss_step=62.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  68%|██████▊   | 21/31 [00:09<00:04,  2.26it/s, loss=81.5, v_num=0, train_loss_step=70.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  71%|███████   | 22/31 [00:09<00:03,  2.27it/s, loss=81.5, v_num=0, train_loss_step=70.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  71%|███████   | 22/31 [00:09<00:03,  2.27it/s, loss=81.2, v_num=0, train_loss_step=55.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  74%|███████▍  | 23/31 [00:10<00:03,  2.28it/s, loss=81.2, v_num=0, train_loss_step=55.40, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  74%|███████▍  | 23/31 [00:10<00:03,  2.28it/s, loss=80.6, v_num=0, train_loss_step=81.20, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  77%|███████▋  | 24/31 [00:10<00:03,  2.28it/s, loss=80.6, v_num=0, train_loss_step=81.20, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  77%|███████▋  | 24/31 [00:10<00:03,  2.28it/s, loss=80.6, v_num=0, train_loss_step=73.60, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  81%|████████  | 25/31 [00:10<00:02,  2.29it/s, loss=80.6, v_num=0, train_loss_step=73.60, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  81%|████████  | 25/31 [00:10<00:02,  2.29it/s, loss=80.7, v_num=0, train_loss_step=90.00, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  84%|████████▍ | 26/31 [00:11<00:02,  2.29it/s, loss=80.7, v_num=0, train_loss_step=90.00, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  84%|████████▍ | 26/31 [00:11<00:02,  2.29it/s, loss=80.9, v_num=0, train_loss_step=97.00, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  87%|████████▋ | 27/31 [00:11<00:01,  2.29it/s, loss=80.9, v_num=0, train_loss_step=97.00, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  87%|████████▋ | 27/31 [00:11<00:01,  2.29it/s, loss=78.2, v_num=0, train_loss_step=52.80, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  90%|█████████ | 28/31 [00:12<00:01,  2.29it/s, loss=78.2, v_num=0, train_loss_step=52.80, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  90%|█████████ | 28/31 [00:12<00:01,  2.29it/s, loss=77.6, v_num=0, train_loss_step=81.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  94%|█████████▎| 29/31 [00:12<00:00,  2.30it/s, loss=77.6, v_num=0, train_loss_step=81.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  94%|█████████▎| 29/31 [00:12<00:00,  2.30it/s, loss=77.5, v_num=0, train_loss_step=78.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  97%|█████████▋| 30/31 [00:13<00:00,  2.30it/s, loss=77.5, v_num=0, train_loss_step=78.10, val_loss=125.0, train_loss_epoch=117.0]#015Epoch 1:  97%|█████████▋| 30/31 [00:13<00:00,  2.30it/s, loss=76.4, v_num=0, train_loss_step=58.00, val_loss=125.0, train_loss_epoch=117.0]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]#033[A#015Epoch 1: 100%|██████████| 31/31 [00:14<00:00,  2.12it/s, loss=76.4, v_num=0, train_loss_step=58.00, val_loss=104.0, train_loss_epoch=117.0]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015                                                         #033[A#015Epoch 1: 100%|██████████| 31/31 [00:15<00:00,  1.97it/s, loss=76.4, v_num=0, train_loss_step=58.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 1:   0%|          | 0/31 [00:00<?, ?it/s, loss=76.4, v_num=0, train_loss_step=58.00, val_loss=104.0, train_loss_epoch=78.60]         #015Epoch 2:   0%|          | 0/31 [00:00<?, ?it/s, loss=76.4, v_num=0, train_loss_step=58.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, loss=76.4, v_num=0, train_loss_step=58.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, loss=73.9, v_num=0, train_loss_step=51.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:   6%|▋         | 2/31 [00:01<00:16,  1.73it/s, loss=73.9, v_num=0, train_loss_step=51.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:   6%|▋         | 2/31 [00:01<00:16,  1.73it/s, loss=74.3, v_num=0, train_loss_step=72.40, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  10%|▉         | 3/31 [00:01<00:14,  1.94it/s, loss=74.3, v_num=0, train_loss_step=72.40, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  10%|▉         | 3/31 [00:01<00:14,  1.94it/s, loss=73.9, v_num=0, train_loss_step=66.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  13%|█▎        | 4/31 [00:01<00:13,  2.05it/s, loss=73.9, v_num=0, train_loss_step=66.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  13%|█▎        | 4/31 [00:01<00:13,  2.05it/s, loss=73.4, v_num=0, train_loss_step=76.70, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  16%|█▌        | 5/31 [00:02<00:12,  2.15it/s, loss=73.4, v_num=0, train_loss_step=76.70, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  16%|█▌        | 5/31 [00:02<00:12,  2.15it/s, loss=72.8, v_num=0, train_loss_step=64.10, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  19%|█▉        | 6/31 [00:02<00:11,  2.19it/s, loss=72.8, v_num=0, train_loss_step=64.10, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  19%|█▉        | 6/31 [00:02<00:11,  2.19it/s, loss=71.8, v_num=0, train_loss_step=67.30, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  23%|██▎       | 7/31 [00:03<00:10,  2.24it/s, loss=71.8, v_num=0, train_loss_step=67.30, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  23%|██▎       | 7/31 [00:03<00:10,  2.23it/s, loss=71, v_num=0, train_loss_step=56.20, val_loss=104.0, train_loss_epoch=78.60]  #015Epoch 2:  26%|██▌       | 8/31 [00:03<00:10,  2.27it/s, loss=71, v_num=0, train_loss_step=56.20, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  26%|██▌       | 8/31 [00:03<00:10,  2.27it/s, loss=70.6, v_num=0, train_loss_step=92.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  29%|██▉       | 9/31 [00:03<00:09,  2.30it/s, loss=70.6, v_num=0, train_loss_step=92.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  29%|██▉       | 9/31 [00:03<00:09,  2.29it/s, loss=71.7, v_num=0, train_loss_step=87.70, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  32%|███▏      | 10/31 [00:04<00:09,  2.31it/s, loss=71.7, v_num=0, train_loss_step=87.70, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  32%|███▏      | 10/31 [00:04<00:09,  2.31it/s, loss=71.4, v_num=0, train_loss_step=56.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  35%|███▌      | 11/31 [00:04<00:09,  2.22it/s, loss=71.4, v_num=0, train_loss_step=56.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  35%|███▌      | 11/31 [00:04<00:09,  2.22it/s, loss=71.6, v_num=0, train_loss_step=74.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  39%|███▊      | 12/31 [00:05<00:08,  2.24it/s, loss=71.6, v_num=0, train_loss_step=74.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  39%|███▊      | 12/31 [00:05<00:08,  2.24it/s, loss=72.6, v_num=0, train_loss_step=76.10, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  42%|████▏     | 13/31 [00:05<00:07,  2.26it/s, loss=72.6, v_num=0, train_loss_step=76.10, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  42%|████▏     | 13/31 [00:05<00:07,  2.26it/s, loss=72.2, v_num=0, train_loss_step=73.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  45%|████▌     | 14/31 [00:06<00:07,  2.28it/s, loss=72.2, v_num=0, train_loss_step=73.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  45%|████▌     | 14/31 [00:06<00:07,  2.28it/s, loss=72.2, v_num=0, train_loss_step=72.40, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  48%|████▊     | 15/31 [00:06<00:06,  2.30it/s, loss=72.2, v_num=0, train_loss_step=72.40, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  48%|████▊     | 15/31 [00:06<00:06,  2.30it/s, loss=71.8, v_num=0, train_loss_step=82.90, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  52%|█████▏    | 16/31 [00:06<00:06,  2.32it/s, loss=71.8, v_num=0, train_loss_step=82.90, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  52%|█████▏    | 16/31 [00:06<00:06,  2.32it/s, loss=71.1, v_num=0, train_loss_step=83.20, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  55%|█████▍    | 17/31 [00:07<00:06,  2.33it/s, loss=71.1, v_num=0, train_loss_step=83.20, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  55%|█████▍    | 17/31 [00:07<00:06,  2.33it/s, loss=71.9, v_num=0, train_loss_step=68.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  58%|█████▊    | 18/31 [00:07<00:05,  2.34it/s, loss=71.9, v_num=0, train_loss_step=68.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  58%|█████▊    | 18/31 [00:07<00:05,  2.34it/s, loss=71.4, v_num=0, train_loss_step=71.10, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  61%|██████▏   | 19/31 [00:08<00:05,  2.35it/s, loss=71.4, v_num=0, train_loss_step=71.10, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  61%|██████▏   | 19/31 [00:08<00:05,  2.35it/s, loss=71.2, v_num=0, train_loss_step=75.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  65%|██████▍   | 20/31 [00:08<00:04,  2.36it/s, loss=71.2, v_num=0, train_loss_step=75.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  65%|██████▍   | 20/31 [00:08<00:04,  2.36it/s, loss=73.9, v_num=0, train_loss_step=110.0, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  68%|██████▊   | 21/31 [00:09<00:04,  2.30it/s, loss=73.9, v_num=0, train_loss_step=110.0, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  68%|██████▊   | 21/31 [00:09<00:04,  2.30it/s, loss=74.4, v_num=0, train_loss_step=61.30, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  71%|███████   | 22/31 [00:09<00:03,  2.30it/s, loss=74.4, v_num=0, train_loss_step=61.30, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  71%|███████   | 22/31 [00:09<00:03,  2.30it/s, loss=75, v_num=0, train_loss_step=84.10, val_loss=104.0, train_loss_epoch=78.60]  #015Epoch 2:  74%|███████▍  | 23/31 [00:09<00:03,  2.31it/s, loss=75, v_num=0, train_loss_step=84.10, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  74%|███████▍  | 23/31 [00:09<00:03,  2.31it/s, loss=75.4, v_num=0, train_loss_step=75.30, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  77%|███████▋  | 24/31 [00:10<00:03,  2.31it/s, loss=75.4, v_num=0, train_loss_step=75.30, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  77%|███████▋  | 24/31 [00:10<00:03,  2.31it/s, loss=75.5, v_num=0, train_loss_step=77.90, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  81%|████████  | 25/31 [00:10<00:02,  2.32it/s, loss=75.5, v_num=0, train_loss_step=77.90, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  81%|████████  | 25/31 [00:10<00:02,  2.32it/s, loss=75.9, v_num=0, train_loss_step=73.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  84%|████████▍ | 26/31 [00:11<00:02,  2.32it/s, loss=75.9, v_num=0, train_loss_step=73.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  84%|████████▍ | 26/31 [00:11<00:02,  2.32it/s, loss=76.8, v_num=0, train_loss_step=84.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  87%|████████▋ | 27/31 [00:11<00:01,  2.32it/s, loss=76.8, v_num=0, train_loss_step=84.00, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  87%|████████▋ | 27/31 [00:11<00:01,  2.32it/s, loss=76.7, v_num=0, train_loss_step=54.70, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  90%|█████████ | 28/31 [00:12<00:01,  2.32it/s, loss=76.7, v_num=0, train_loss_step=54.70, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  90%|█████████ | 28/31 [00:12<00:01,  2.32it/s, loss=76.1, v_num=0, train_loss_step=81.20, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  94%|█████████▎| 29/31 [00:12<00:00,  2.33it/s, loss=76.1, v_num=0, train_loss_step=81.20, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  94%|█████████▎| 29/31 [00:12<00:00,  2.33it/s, loss=74.9, v_num=0, train_loss_step=63.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  97%|█████████▋| 30/31 [00:12<00:00,  2.33it/s, loss=74.9, v_num=0, train_loss_step=63.50, val_loss=104.0, train_loss_epoch=78.60]#015Epoch 2:  97%|█████████▋| 30/31 [00:12<00:00,  2.32it/s, loss=75.6, v_num=0, train_loss_step=69.60, val_loss=104.0, train_loss_epoch=78.60]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]#033[A#015Epoch 2: 100%|██████████| 31/31 [00:14<00:00,  2.10it/s, loss=75.6, v_num=0, train_loss_step=69.60, val_loss=102.0, train_loss_epoch=78.60]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015                                                         #033[A#015Epoch 2: 100%|██████████| 31/31 [00:15<00:00,  1.99it/s, loss=75.6, v_num=0, train_loss_step=69.60, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 2:   0%|          | 0/31 [00:00<?, ?it/s, loss=75.6, v_num=0, train_loss_step=69.60, val_loss=102.0, train_loss_epoch=73.40]         #015Epoch 3:   0%|          | 0/31 [00:00<?, ?it/s, loss=75.6, v_num=0, train_loss_step=69.60, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:   3%|▎         | 1/31 [00:00<00:26,  1.15it/s, loss=75.6, v_num=0, train_loss_step=69.60, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:   3%|▎         | 1/31 [00:00<00:26,  1.15it/s, loss=76.3, v_num=0, train_loss_step=88.70, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:   6%|▋         | 2/31 [00:01<00:18,  1.61it/s, loss=76.3, v_num=0, train_loss_step=88.70, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:   6%|▋         | 2/31 [00:01<00:18,  1.61it/s, loss=77, v_num=0, train_loss_step=90.70, val_loss=102.0, train_loss_epoch=73.40]  #015Epoch 3:  10%|▉         | 3/31 [00:01<00:15,  1.85it/s, loss=77, v_num=0, train_loss_step=90.70, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  10%|▉         | 3/31 [00:01<00:15,  1.85it/s, loss=77.6, v_num=0, train_loss_step=84.20, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  13%|█▎        | 4/31 [00:02<00:13,  1.98it/s, loss=77.6, v_num=0, train_loss_step=84.20, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  13%|█▎        | 4/31 [00:02<00:13,  1.98it/s, loss=77.1, v_num=0, train_loss_step=62.10, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  16%|█▌        | 5/31 [00:02<00:12,  2.05it/s, loss=77.1, v_num=0, train_loss_step=62.10, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  16%|█▌        | 5/31 [00:02<00:12,  2.05it/s, loss=77.8, v_num=0, train_loss_step=97.40, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  19%|█▉        | 6/31 [00:02<00:12,  2.08it/s, loss=77.8, v_num=0, train_loss_step=97.40, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  19%|█▉        | 6/31 [00:02<00:12,  2.08it/s, loss=77.5, v_num=0, train_loss_step=76.90, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  23%|██▎       | 7/31 [00:03<00:11,  2.15it/s, loss=77.5, v_num=0, train_loss_step=76.90, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  23%|██▎       | 7/31 [00:03<00:11,  2.15it/s, loss=77.7, v_num=0, train_loss_step=72.60, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  26%|██▌       | 8/31 [00:03<00:10,  2.20it/s, loss=77.7, v_num=0, train_loss_step=72.60, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  26%|██▌       | 8/31 [00:03<00:10,  2.20it/s, loss=77.8, v_num=0, train_loss_step=72.80, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  29%|██▉       | 9/31 [00:04<00:09,  2.24it/s, loss=77.8, v_num=0, train_loss_step=72.80, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  29%|██▉       | 9/31 [00:04<00:09,  2.24it/s, loss=77.2, v_num=0, train_loss_step=63.40, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  32%|███▏      | 10/31 [00:04<00:09,  2.27it/s, loss=77.2, v_num=0, train_loss_step=63.40, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  32%|███▏      | 10/31 [00:04<00:09,  2.27it/s, loss=76.2, v_num=0, train_loss_step=90.30, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  35%|███▌      | 11/31 [00:05<00:09,  2.17it/s, loss=76.2, v_num=0, train_loss_step=90.30, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  35%|███▌      | 11/31 [00:05<00:09,  2.17it/s, loss=75.8, v_num=0, train_loss_step=53.20, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  39%|███▊      | 12/31 [00:05<00:08,  2.18it/s, loss=75.8, v_num=0, train_loss_step=53.20, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  39%|███▊      | 12/31 [00:05<00:08,  2.18it/s, loss=75.9, v_num=0, train_loss_step=86.70, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  42%|████▏     | 13/31 [00:05<00:08,  2.20it/s, loss=75.9, v_num=0, train_loss_step=86.70, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  42%|████▏     | 13/31 [00:05<00:08,  2.20it/s, loss=75.5, v_num=0, train_loss_step=66.10, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  45%|████▌     | 14/31 [00:06<00:07,  2.20it/s, loss=75.5, v_num=0, train_loss_step=66.10, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  45%|████▌     | 14/31 [00:06<00:07,  2.20it/s, loss=74.5, v_num=0, train_loss_step=58.80, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  48%|████▊     | 15/31 [00:06<00:07,  2.22it/s, loss=74.5, v_num=0, train_loss_step=58.80, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  48%|████▊     | 15/31 [00:06<00:07,  2.22it/s, loss=74.4, v_num=0, train_loss_step=70.80, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  52%|█████▏    | 16/31 [00:07<00:06,  2.23it/s, loss=74.4, v_num=0, train_loss_step=70.80, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  52%|█████▏    | 16/31 [00:07<00:06,  2.23it/s, loss=73.2, v_num=0, train_loss_step=60.40, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  55%|█████▍    | 17/31 [00:07<00:06,  2.24it/s, loss=73.2, v_num=0, train_loss_step=60.40, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  55%|█████▍    | 17/31 [00:07<00:06,  2.24it/s, loss=74.5, v_num=0, train_loss_step=79.90, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  58%|█████▊    | 18/31 [00:08<00:05,  2.25it/s, loss=74.5, v_num=0, train_loss_step=79.90, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  58%|█████▊    | 18/31 [00:08<00:05,  2.24it/s, loss=73.8, v_num=0, train_loss_step=67.50, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  61%|██████▏   | 19/31 [00:08<00:05,  2.25it/s, loss=73.8, v_num=0, train_loss_step=67.50, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  61%|██████▏   | 19/31 [00:08<00:05,  2.25it/s, loss=73.9, v_num=0, train_loss_step=65.70, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  65%|██████▍   | 20/31 [00:08<00:04,  2.26it/s, loss=73.9, v_num=0, train_loss_step=65.70, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  65%|██████▍   | 20/31 [00:08<00:04,  2.26it/s, loss=74.1, v_num=0, train_loss_step=73.50, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  68%|██████▊   | 21/31 [00:09<00:04,  2.21it/s, loss=74.1, v_num=0, train_loss_step=73.50, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  68%|██████▊   | 21/31 [00:09<00:04,  2.21it/s, loss=72.8, v_num=0, train_loss_step=62.60, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  71%|███████   | 22/31 [00:09<00:04,  2.22it/s, loss=72.8, v_num=0, train_loss_step=62.60, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  71%|███████   | 22/31 [00:09<00:04,  2.22it/s, loss=71.9, v_num=0, train_loss_step=72.50, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  74%|███████▍  | 23/31 [00:10<00:03,  2.23it/s, loss=71.9, v_num=0, train_loss_step=72.50, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  74%|███████▍  | 23/31 [00:10<00:03,  2.23it/s, loss=72.2, v_num=0, train_loss_step=91.80, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  77%|███████▋  | 24/31 [00:10<00:03,  2.23it/s, loss=72.2, v_num=0, train_loss_step=91.80, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  77%|███████▋  | 24/31 [00:10<00:03,  2.23it/s, loss=72.2, v_num=0, train_loss_step=62.00, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  81%|████████  | 25/31 [00:11<00:02,  2.24it/s, loss=72.2, v_num=0, train_loss_step=62.00, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  81%|████████  | 25/31 [00:11<00:02,  2.24it/s, loss=70.2, v_num=0, train_loss_step=56.20, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  84%|████████▍ | 26/31 [00:11<00:02,  2.24it/s, loss=70.2, v_num=0, train_loss_step=56.20, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  84%|████████▍ | 26/31 [00:11<00:02,  2.24it/s, loss=69, v_num=0, train_loss_step=52.70, val_loss=102.0, train_loss_epoch=73.40]  #015Epoch 3:  87%|████████▋ | 27/31 [00:12<00:01,  2.25it/s, loss=69, v_num=0, train_loss_step=52.70, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  87%|████████▋ | 27/31 [00:12<00:01,  2.25it/s, loss=68.5, v_num=0, train_loss_step=62.60, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  90%|█████████ | 28/31 [00:12<00:01,  2.25it/s, loss=68.5, v_num=0, train_loss_step=62.60, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  90%|█████████ | 28/31 [00:12<00:01,  2.25it/s, loss=68.6, v_num=0, train_loss_step=75.40, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  94%|█████████▎| 29/31 [00:12<00:00,  2.26it/s, loss=68.6, v_num=0, train_loss_step=75.40, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  94%|█████████▎| 29/31 [00:12<00:00,  2.26it/s, loss=69, v_num=0, train_loss_step=70.80, val_loss=102.0, train_loss_epoch=73.40]  #015Epoch 3:  97%|█████████▋| 30/31 [00:13<00:00,  2.26it/s, loss=69, v_num=0, train_loss_step=70.80, val_loss=102.0, train_loss_epoch=73.40]#015Epoch 3:  97%|█████████▋| 30/31 [00:13<00:00,  2.26it/s, loss=67.2, v_num=0, train_loss_step=55.20, val_loss=102.0, train_loss_epoch=73.40]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]#033[A#015Epoch 3: 100%|██████████| 31/31 [00:14<00:00,  2.08it/s, loss=67.2, v_num=0, train_loss_step=55.20, val_loss=91.80, train_loss_epoch=73.40]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015                                                         #033[A#015Epoch 3: 100%|██████████| 31/31 [00:15<00:00,  1.94it/s, loss=67.2, v_num=0, train_loss_step=55.20, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 3:   0%|          | 0/31 [00:00<?, ?it/s, loss=67.2, v_num=0, train_loss_step=55.20, val_loss=91.80, train_loss_epoch=71.40]         #015Epoch 4:   0%|          | 0/31 [00:00<?, ?it/s, loss=67.2, v_num=0, train_loss_step=55.20, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, loss=67.2, v_num=0, train_loss_step=55.20, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, loss=67.7, v_num=0, train_loss_step=63.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:   6%|▋         | 2/31 [00:01<00:17,  1.67it/s, loss=67.7, v_num=0, train_loss_step=63.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:   6%|▋         | 2/31 [00:01<00:17,  1.67it/s, loss=66.4, v_num=0, train_loss_step=60.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  10%|▉         | 3/31 [00:01<00:14,  1.91it/s, loss=66.4, v_num=0, train_loss_step=60.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  10%|▉         | 3/31 [00:01<00:14,  1.91it/s, loss=66.5, v_num=0, train_loss_step=68.10, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  13%|█▎        | 4/31 [00:01<00:13,  2.03it/s, loss=66.5, v_num=0, train_loss_step=68.10, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  13%|█▎        | 4/31 [00:01<00:13,  2.02it/s, loss=66.6, v_num=0, train_loss_step=61.30, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  16%|█▌        | 5/31 [00:02<00:12,  2.12it/s, loss=66.6, v_num=0, train_loss_step=61.30, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  16%|█▌        | 5/31 [00:02<00:12,  2.12it/s, loss=66.3, v_num=0, train_loss_step=63.60, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  19%|█▉        | 6/31 [00:02<00:11,  2.18it/s, loss=66.3, v_num=0, train_loss_step=63.60, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  19%|█▉        | 6/31 [00:02<00:11,  2.18it/s, loss=66.7, v_num=0, train_loss_step=68.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  23%|██▎       | 7/31 [00:03<00:10,  2.23it/s, loss=66.7, v_num=0, train_loss_step=68.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  23%|██▎       | 7/31 [00:03<00:10,  2.23it/s, loss=65.3, v_num=0, train_loss_step=51.80, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  26%|██▌       | 8/31 [00:03<00:10,  2.27it/s, loss=65.3, v_num=0, train_loss_step=51.80, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  26%|██▌       | 8/31 [00:03<00:10,  2.27it/s, loss=64.8, v_num=0, train_loss_step=58.80, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  29%|██▉       | 9/31 [00:03<00:09,  2.29it/s, loss=64.8, v_num=0, train_loss_step=58.80, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  29%|██▉       | 9/31 [00:03<00:09,  2.29it/s, loss=63.7, v_num=0, train_loss_step=42.80, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  32%|███▏      | 10/31 [00:04<00:09,  2.30it/s, loss=63.7, v_num=0, train_loss_step=42.80, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  32%|███▏      | 10/31 [00:04<00:09,  2.30it/s, loss=62.9, v_num=0, train_loss_step=58.00, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  35%|███▌      | 11/31 [00:05<00:09,  2.20it/s, loss=62.9, v_num=0, train_loss_step=58.00, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  35%|███▌      | 11/31 [00:05<00:09,  2.20it/s, loss=62.9, v_num=0, train_loss_step=62.30, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  39%|███▊      | 12/31 [00:05<00:08,  2.22it/s, loss=62.9, v_num=0, train_loss_step=62.30, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  39%|███▊      | 12/31 [00:05<00:08,  2.22it/s, loss=61.4, v_num=0, train_loss_step=42.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  42%|████▏     | 13/31 [00:05<00:08,  2.24it/s, loss=61.4, v_num=0, train_loss_step=42.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  42%|████▏     | 13/31 [00:05<00:08,  2.24it/s, loss=59.9, v_num=0, train_loss_step=61.20, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  45%|████▌     | 14/31 [00:06<00:07,  2.25it/s, loss=59.9, v_num=0, train_loss_step=61.20, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  45%|████▌     | 14/31 [00:06<00:07,  2.25it/s, loss=59.6, v_num=0, train_loss_step=55.70, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  48%|████▊     | 15/31 [00:06<00:07,  2.27it/s, loss=59.6, v_num=0, train_loss_step=55.70, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  48%|████▊     | 15/31 [00:06<00:07,  2.27it/s, loss=59.8, v_num=0, train_loss_step=60.60, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  52%|█████▏    | 16/31 [00:07<00:06,  2.28it/s, loss=59.8, v_num=0, train_loss_step=60.60, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  52%|█████▏    | 16/31 [00:07<00:06,  2.28it/s, loss=60, v_num=0, train_loss_step=56.80, val_loss=91.80, train_loss_epoch=71.40]  #015Epoch 4:  55%|█████▍    | 17/31 [00:07<00:06,  2.29it/s, loss=60, v_num=0, train_loss_step=56.80, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  55%|█████▍    | 17/31 [00:07<00:06,  2.29it/s, loss=59.8, v_num=0, train_loss_step=58.10, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  58%|█████▊    | 18/31 [00:07<00:05,  2.30it/s, loss=59.8, v_num=0, train_loss_step=58.10, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  58%|█████▊    | 18/31 [00:07<00:05,  2.30it/s, loss=58.9, v_num=0, train_loss_step=57.00, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  61%|██████▏   | 19/31 [00:08<00:05,  2.32it/s, loss=58.9, v_num=0, train_loss_step=57.00, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  61%|██████▏   | 19/31 [00:08<00:05,  2.32it/s, loss=57.7, v_num=0, train_loss_step=47.90, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  65%|██████▍   | 20/31 [00:08<00:04,  2.33it/s, loss=57.7, v_num=0, train_loss_step=47.90, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  65%|██████▍   | 20/31 [00:08<00:04,  2.33it/s, loss=58.9, v_num=0, train_loss_step=79.70, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  68%|██████▊   | 21/31 [00:09<00:04,  2.29it/s, loss=58.9, v_num=0, train_loss_step=79.70, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  68%|██████▊   | 21/31 [00:09<00:04,  2.29it/s, loss=59.5, v_num=0, train_loss_step=74.00, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  71%|███████   | 22/31 [00:09<00:03,  2.30it/s, loss=59.5, v_num=0, train_loss_step=74.00, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  71%|███████   | 22/31 [00:09<00:03,  2.30it/s, loss=59.8, v_num=0, train_loss_step=67.40, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  74%|███████▍  | 23/31 [00:09<00:03,  2.31it/s, loss=59.8, v_num=0, train_loss_step=67.40, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  74%|███████▍  | 23/31 [00:09<00:03,  2.31it/s, loss=59, v_num=0, train_loss_step=51.10, val_loss=91.80, train_loss_epoch=71.40]  #015Epoch 4:  77%|███████▋  | 24/31 [00:10<00:03,  2.32it/s, loss=59, v_num=0, train_loss_step=51.10, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  77%|███████▋  | 24/31 [00:10<00:03,  2.32it/s, loss=58.9, v_num=0, train_loss_step=60.70, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  81%|████████  | 25/31 [00:10<00:02,  2.33it/s, loss=58.9, v_num=0, train_loss_step=60.70, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  81%|████████  | 25/31 [00:10<00:02,  2.33it/s, loss=58.9, v_num=0, train_loss_step=62.90, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  84%|████████▍ | 26/31 [00:11<00:02,  2.34it/s, loss=58.9, v_num=0, train_loss_step=62.90, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  84%|████████▍ | 26/31 [00:11<00:02,  2.34it/s, loss=58.4, v_num=0, train_loss_step=58.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  87%|████████▋ | 27/31 [00:11<00:01,  2.34it/s, loss=58.4, v_num=0, train_loss_step=58.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  87%|████████▋ | 27/31 [00:11<00:01,  2.34it/s, loss=58.2, v_num=0, train_loss_step=49.00, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  90%|█████████ | 28/31 [00:11<00:01,  2.35it/s, loss=58.2, v_num=0, train_loss_step=49.00, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  90%|█████████ | 28/31 [00:11<00:01,  2.35it/s, loss=58.8, v_num=0, train_loss_step=70.70, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  94%|█████████▎| 29/31 [00:12<00:00,  2.36it/s, loss=58.8, v_num=0, train_loss_step=70.70, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  94%|█████████▎| 29/31 [00:12<00:00,  2.36it/s, loss=59.8, v_num=0, train_loss_step=61.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  97%|█████████▋| 30/31 [00:12<00:00,  2.36it/s, loss=59.8, v_num=0, train_loss_step=61.50, val_loss=91.80, train_loss_epoch=71.40]#015Epoch 4:  97%|█████████▋| 30/31 [00:12<00:00,  2.36it/s, loss=60.4, v_num=0, train_loss_step=70.50, val_loss=91.80, train_loss_epoch=71.40]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]#033[A#015Epoch 4: 100%|██████████| 31/31 [00:14<00:00,  2.15it/s, loss=60.4, v_num=0, train_loss_step=70.50, val_loss=85.20, train_loss_epoch=71.40]\u001b[0m\n",
      "\u001b[34m#015                                                         #033[A#015Epoch 4: 100%|██████████| 31/31 [00:15<00:00,  2.03it/s, loss=60.4, v_num=0, train_loss_step=70.50, val_loss=85.20, train_loss_epoch=60.20]#015Epoch 4: 100%|██████████| 31/31 [00:15<00:00,  1.99it/s, loss=60.4, v_num=0, train_loss_step=70.50, val_loss=85.20, train_loss_epoch=60.20]\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "   | Name                               | Type                            | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m0  | loss                               | QuantileLoss                    | 0     \u001b[0m\n",
      "\u001b[34m1  | logging_metrics                    | ModuleList                      | 0     \u001b[0m\n",
      "\u001b[34m2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \u001b[0m\n",
      "\u001b[34m3  | prescalers                         | ModuleDict                      | 256   \u001b[0m\n",
      "\u001b[34m4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \u001b[0m\n",
      "\u001b[34m5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \u001b[0m\n",
      "\u001b[34m6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \u001b[0m\n",
      "\u001b[34m7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m11 | lstm_encoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m12 | lstm_decoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \u001b[0m\n",
      "\u001b[34m14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \u001b[0m\n",
      "\u001b[34m15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \u001b[0m\n",
      "\u001b[34m2022-08-21 17:36:37,523 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \u001b[0m\n",
      "\u001b[34m17 | post_attn_gate_norm                | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m19 | pre_output_gate_norm               | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m20 | output_layer                       | Linear                          | 119   \u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m29.7 K    Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m29.7 K    Total params\u001b[0m\n",
      "\u001b[34m0.119     Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 350. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 128. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-08-21 17:36:51 Uploading - Uploading generated training model\n",
      "2022-08-21 17:37:11 Completed - Training job completed\n",
      "ProfilerReport-1661102926: NoIssuesFound\n",
      "Training seconds: 378\n",
      "Billable seconds: 378\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "        'epochs': 5,\n",
    "        'data-filename': \"stallion_data.parquet\",\n",
    "        'metadata-filename': \"stallion_metadata.json\"\n",
    "    }\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path='s3://sagemaker-us-east-1-551329315830/tensorboard',\n",
    "    container_local_output_path='/lightning_logs'\n",
    ")\n",
    "\n",
    "spot_estimator  = PyTorch(entry_point='TFT_docker/TFT.py',\n",
    "                            dependencies=['TFT_docker/requirements.txt'],\n",
    "                            role=role,\n",
    "                            framework_version='1.7.1',\n",
    "                            py_version='py3',\n",
    "                            instance_count=1,\n",
    "#                             instance_type='local',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "#                             instance_type='ml.p2.xlarge',\n",
    "                            base_job_name='tft-pytorch-spot-1',\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            checkpoint_s3_uri=checkpoint_s3_path,\n",
    "                            debugger_hook_config=False,\n",
    "                            input_mode = 'FastFile',\n",
    "                            use_spot_instances=use_spot_instances,\n",
    "                            max_run=max_run,\n",
    "                            max_wait=max_wait,\n",
    "                            tensorboard_output_config=tensorboard_output_config\n",
    "                           )\n",
    "\n",
    "spot_estimator.fit(\n",
    "                inputs,\n",
    "                logs = 'All'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cb7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b317ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_estimator.latest_job_tensorboard_artifacts_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b009a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deploy the trained model\n",
    "# predictor=estimator.deploy(1, instance_type)\n",
    "tensorflow_logs_path = \"lightning_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63848445",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region = sagemaker_session.boto_region_name\n",
    "!AWS_REGION={aws_region}\n",
    "!echo tensorboard --logdir {tensorflow_logs_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4f277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.0 at http://localhost:6007/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!AWS_REGION=eu-east-1 tensorboard --logdir s3://sagemaker-us-east-1-551329315830/tensorboard/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
