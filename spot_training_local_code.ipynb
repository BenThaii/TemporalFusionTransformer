{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a4bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'551329315830.dkr.ecr.us-east-1.amazonaws.com/pytorch-tft-container-test:latest'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client=boto3.client('sts')\n",
    "account=client.get_caller_identity()['Account']\n",
    "\n",
    "my_session=boto3.session.Session()\n",
    "region=my_session.region_name\n",
    "\n",
    "algorithm_name=\"pytorch-tft-container-test\"\n",
    "ecr_image='{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "\n",
    "ecr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27eafa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\r\n",
      "Configure a credential helper to remove this warning. See\r\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\r\n",
      "\r\n",
      "Login Succeeded\r\n"
     ]
    }
   ],
   "source": [
    "! aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51fe196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker version: 2.106.0\n",
      "Checkpointing Path: s3://sagemaker-us-east-1-551329315830/checkpoints/checkpoint-86bcdfa1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import uuid\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "print('SageMaker version: ' + sagemaker.__version__)\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-cnn-cifar10'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_path = 's3://{}/checkpoints/checkpoint-{}'.format(bucket, checkpoint_suffix)\n",
    "\n",
    "print('Checkpointing Path: {}'.format(checkpoint_s3_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99537435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointing directory timeseries_data exists\n",
      "saved raw data to timeseries_data/stallion_data.parquet\n",
      "Checkpointing directory timeseries_data exists\n",
      "saved metadata to timeseries_data/stallion_metadata.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-551329315830/data/timeseries_data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_timeseries import download_process_and_return_raw_data, save_local_and_upload_s3, metadata_json_upload_s3\n",
    "import sagemaker\n",
    "import uuid\n",
    "\n",
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "special_days = [\n",
    "        \"easter_day\",\n",
    "        \"good_friday\",\n",
    "        \"new_year\",\n",
    "        \"christmas\",\n",
    "        \"labor_day\",\n",
    "        \"independence_day\",\n",
    "        \"revolution_day_memorial\",\n",
    "        \"regional_games\",\n",
    "        \"fifa_u_17_world_cup\",\n",
    "        \"football_gold_cup\",\n",
    "        \"beer_capital\",\n",
    "        \"music_fest\",\n",
    "    ]\n",
    "\n",
    "training_metadata = {}\n",
    "training_metadata['time_idx'] = \"time_idx\"\n",
    "training_metadata['target'] = \"volume\"\n",
    "training_metadata['group_ids'] = [\"agency\", \"sku\"]\n",
    "training_metadata['min_encoder_length'] = max_encoder_length // 2      # keep encoder length long (as it is in the validation set)\n",
    "training_metadata['max_encoder_length'] = max_encoder_length\n",
    "training_metadata['min_prediction_length'] = 1      \n",
    "training_metadata['max_prediction_length'] = max_prediction_length\n",
    "training_metadata['static_categoricals'] = [\"agency\", \"sku\"]\n",
    "training_metadata['static_reals'] = [\"avg_population_2017\", \"avg_yearly_household_income_2017\"]\n",
    "training_metadata['time_varying_known_categoricals'] = [\"special_days\", \"month\"]\n",
    "training_metadata['variable_groups'] = {\"special_days\": special_days}\n",
    "training_metadata['time_varying_known_reals'] = [\"time_idx\", \"price_regular\", \"discount_in_percent\"]\n",
    "training_metadata['time_varying_unknown_categoricals'] = []\n",
    "training_metadata['time_varying_unknown_reals'] = [\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ]\n",
    "training_metadata['target_normalizer'] = {\n",
    "                            \"normalized_groups\": [\"agency\", \"sku\"],\n",
    "                            \"normalization_transformation\": 'softplus'\n",
    "                        }\n",
    "training_metadata['add_relative_time_idx'] = True\n",
    "training_metadata['add_target_scales'] = True\n",
    "training_metadata['add_encoder_length'] = True\n",
    "training_metadata['allow_missing_timesteps'] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# obtain & upload training data\n",
    "training_data = download_process_and_return_raw_data()\n",
    "inputs = save_local_and_upload_s3(training_data, sagemaker_session, bucket, data_filename=\"stallion_data\")\n",
    "\n",
    "# upload metadata\n",
    "training_metadata['training_cutoff'] = int(training_data[\"time_idx\"].max() - max_prediction_length)\n",
    "metadata_json_upload_s3(training_metadata, sagemaker_session, bucket, metadata_filename=\"stallion_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9527f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot_instances = False\n",
    "max_run=6000      # in seconds, after this, job will be terminated\n",
    "max_wait = 10 * max_run if use_spot_instances else None\n",
    "local_image_name = 'pytorch-tft-container-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b937240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be44e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-03 08:56:16 Starting - Starting the training job...ProfilerReport-1662195376: InProgress\n",
      "...\n",
      "2022-09-03 08:57:06 Starting - Preparing the instances for training......\n",
      "2022-09-03 08:58:17 Downloading - Downloading input data......\n",
      "2022-09-03 08:59:07 Training - Downloading the training image...........................\n",
      "2022-09-03 09:03:48 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-03 09:03:42,423 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-03 09:03:42,460 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-03 09:03:42,468 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-03 09:03:43,038 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting pytorch-forecasting\n",
      "  Downloading pytorch_forecasting-0.9.0-py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34mCollecting fastparquet\n",
      "  Downloading fastparquet-0.8.0.tar.gz (400 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (4.0.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning<2.0.0,>=1.2.4\n",
      "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\u001b[0m\n",
      "\u001b[34mCollecting statsmodels\n",
      "  Downloading statsmodels-0.12.2-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<2.0.0,>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn<0.25,>=0.23 in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[34mCollecting optuna<3.0.0,>=2.3.0\n",
      "  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (3.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.4.40-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\u001b[0m\n",
      "\u001b[34mCollecting cliff\n",
      "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mCollecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas<2.0.0,>=1.1.0->pytorch-forecasting->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas<2.0.0,>=1.1.0->pytorch-forecasting->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas<2.0.0,>=1.1.0->pytorch-forecasting->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics>=0.4.1\n",
      "  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[34mCollecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 4)) (3.17.1)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 4)) (0.35.1)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.48.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn<0.25,>=0.23->pytorch-forecasting->-r requirements.txt (line 1)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn<0.25,>=0.23->pytorch-forecasting->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch<2.0,>=1.7->pytorch-forecasting->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.5.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270 kB)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (191 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (21.2.0)\u001b[0m\n",
      "\u001b[34mCollecting idna-ssl>=1.0\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mCollecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mCollecting Mako\n",
      "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34mCollecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34mCollecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-2.5.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34mCollecting autopage>=0.4.0\n",
      "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (0.2.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.6/site-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pytorch-forecasting->-r requirements.txt (line 1)) (8.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pytorch-forecasting->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pytorch-forecasting->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mCollecting patsy>=0.5\n",
      "  Downloading patsy-0.5.2-py2.py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fastparquet, idna-ssl, pyperclip\n",
      "  Building wheel for fastparquet (setup.py): started\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m  Building wheel for fastparquet (setup.py): finished with status 'done'\n",
      "  Created wheel for fastparquet: filename=fastparquet-0.8.0-cp36-cp36m-linux_x86_64.whl size=1256923 sha256=12b99739af3c5431ab49d23762d118b1015ded65dd6bfe0bf0842d3e7689fd82\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/6a/4f/0fd8e8bcbc4b5b751186e363b5b03975d8643eee2975eed2ca\n",
      "  Building wheel for idna-ssl (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for idna-ssl (setup.py): finished with status 'done'\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=48a23731c24fa95de7ebb979e64f542d43ee1f910b09758b644057db9077e55d\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "  Building wheel for pyperclip (setup.py): started\n",
      "  Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11106 sha256=c229c4e55b93187bef318da131717f1de1cfd2242a2fea78e94be8601c2ec2b3\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/95/e30a7f0b44cb90642de3469f211a3218f93f871789b4f4b46c\u001b[0m\n",
      "\u001b[34mSuccessfully built fastparquet idna-ssl pyperclip\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyasn1-modules, oauthlib, multidict, frozenlist, cachetools, yarl, requests-oauthlib, pyperclip, pbr, importlib-metadata, idna-ssl, google-auth, charset-normalizer, asynctest, async-timeout, aiosignal, tensorboard-plugin-wit, tensorboard-data-server, stevedore, sqlalchemy, setuptools, pyDeprecate, PrettyTable, markdown, Mako, importlib-resources, grpcio, google-auth-oauthlib, cmd2, autopage, aiohttp, absl-py, torchmetrics, tensorboard, patsy, colorlog, cmaes, cliff, alembic, statsmodels, pytorch-lightning, optuna, cramjam, pytorch-forecasting, fastparquet\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.0.1\n",
      "    Uninstalling importlib-metadata-4.0.1:\n",
      "      Successfully uninstalled importlib-metadata-4.0.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 49.6.0.post20210108\n",
      "    Uninstalling setuptools-49.6.0.post20210108:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled setuptools-49.6.0.post20210108\u001b[0m\n",
      "\u001b[34mSuccessfully installed Mako-1.1.6 PrettyTable-2.5.0 absl-py-1.2.0 aiohttp-3.8.1 aiosignal-1.2.0 alembic-1.7.7 async-timeout-4.0.2 asynctest-0.13.0 autopage-0.5.1 cachetools-4.2.4 charset-normalizer-2.1.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 cramjam-2.5.0 fastparquet-0.8.0 frozenlist-1.2.0 google-auth-2.11.0 google-auth-oauthlib-0.4.6 grpcio-1.48.1 idna-ssl-1.1.0 importlib-metadata-4.8.3 importlib-resources-5.4.0 markdown-3.3.7 multidict-5.2.0 oauthlib-3.2.0 optuna-2.10.1 patsy-0.5.2 pbr-5.10.0 pyDeprecate-0.3.1 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.9.0 pytorch-lightning-1.5.10 requests-oauthlib-1.3.1 setuptools-59.5.0 sqlalchemy-1.4.40 statsmodels-0.12.2 stevedore-3.5.0 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.8.2 yarl-1.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-09-03 09:04:32,223 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"data-filename\": \"stallion_data.parquet\",\n",
      "        \"metadata-filename\": \"stallion_metadata.json\",\n",
      "        \"num-epochs\": 3,\n",
      "        \"run-mode\": \"test\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tft-pytorch-spot-1-2022-09-03-08-56-15-855\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-09-03-08-56-15-855/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"TFT\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p2.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p2.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"TFT.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"data-filename\":\"stallion_data.parquet\",\"metadata-filename\":\"stallion_metadata.json\",\"num-epochs\":3,\"run-mode\":\"test\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=TFT.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=TFT\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-09-03-08-56-15-855/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"data-filename\":\"stallion_data.parquet\",\"metadata-filename\":\"stallion_metadata.json\",\"num-epochs\":3,\"run-mode\":\"test\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tft-pytorch-spot-1-2022-09-03-08-56-15-855\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-09-03-08-56-15-855/source/sourcedir.tar.gz\",\"module_name\":\"TFT\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"TFT.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--data-filename\",\"stallion_data.parquet\",\"--metadata-filename\",\"stallion_metadata.json\",\"--num-epochs\",\"3\",\"--run-mode\",\"test\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATA-FILENAME=stallion_data.parquet\u001b[0m\n",
      "\u001b[34mSM_HP_METADATA-FILENAME=stallion_metadata.json\u001b[0m\n",
      "\u001b[34mSM_HP_NUM-EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_RUN-MODE=test\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 TFT.py --data-filename stallion_data.parquet --metadata-filename stallion_metadata.json --num-epochs 3 --run-mode test\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mCheckpointing directory /opt/ml/checkpoints exists\u001b[0m\n",
      "\u001b[34mDevice Type: cuda\u001b[0m\n",
      "\u001b[34mLoad Time Series dataset from S3\u001b[0m\n",
      "\u001b[34mcreating dataloader\u001b[0m\n",
      "\u001b[34mcreate model trainer\u001b[0m\n",
      "\u001b[34mcreate model from dataset\u001b[0m\n",
      "\u001b[34mNumber of parameters in network: 3392.2k\u001b[0m\n",
      "\u001b[34mtraining model\u001b[0m\n",
      "\u001b[34m#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]#015Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.88s/it]#015                                                                      #015#015Training: 0it [00:00, ?it/s]#015Training:   0%|          | 0/39 [00:00<?, ?it/s]#015Epoch 0:   0%|          | 0/39 [00:00<?, ?it/s] #015Epoch 0:   3%|▎         | 1/39 [00:00<00:25,  1.49it/s]#015Epoch 0:   3%|▎         | 1/39 [00:00<00:25,  1.49it/s, loss=11.5, train_loss_step=11.50]#015Epoch 0:   5%|▌         | 2/39 [00:00<00:15,  2.32it/s, loss=11.5, train_loss_step=11.50]#015Epoch 0:   5%|▌         | 2/39 [00:00<00:15,  2.32it/s, loss=111, train_loss_step=211.0] #015Epoch 0:   8%|▊         | 3/39 [00:01<00:12,  2.89it/s, loss=111, train_loss_step=211.0]#015Epoch 0:   8%|▊         | 3/39 [00:01<00:12,  2.89it/s, loss=157, train_loss_step=249.0]#015Epoch 0:  10%|█         | 4/39 [00:01<00:10,  3.29it/s, loss=157, train_loss_step=249.0]#015Epoch 0:  10%|█         | 4/39 [00:01<00:10,  3.29it/s, loss=120, train_loss_step=8.610]#015Epoch 0:  13%|█▎        | 5/39 [00:01<00:09,  3.59it/s, loss=120, train_loss_step=8.610]#015Epoch 0:  13%|█▎        | 5/39 [00:01<00:09,  3.58it/s, loss=167, train_loss_step=354.0]#015Epoch 0:  15%|█▌        | 6/39 [00:01<00:09,  3.66it/s, loss=167, train_loss_step=354.0]#015Epoch 0:  15%|█▌        | 6/39 [00:01<00:09,  3.66it/s, loss=153, train_loss_step=83.90]#015Epoch 0:  18%|█▊        | 7/39 [00:01<00:08,  3.74it/s, loss=153, train_loss_step=83.90]#015Epoch 0:  18%|█▊        | 7/39 [00:01<00:08,  3.74it/s, loss=156, train_loss_step=170.0]#015Epoch 0:  21%|██        | 8/39 [00:02<00:08,  3.73it/s, loss=156, train_loss_step=170.0]#015Epoch 0:  21%|██        | 8/39 [00:02<00:08,  3.73it/s, loss=144, train_loss_step=66.80]#015Epoch 0:  23%|██▎       | 9/39 [00:02<00:08,  3.74it/s, loss=144, train_loss_step=66.80]#015Epoch 0:  23%|██▎       | 9/39 [00:02<00:08,  3.74it/s, loss=130, train_loss_step=17.80]#015Epoch 0:  26%|██▌       | 10/39 [00:02<00:07,  3.82it/s, loss=130, train_loss_step=17.80]#015Epoch 0:  26%|██▌       | 10/39 [00:02<00:07,  3.82it/s, loss=125, train_loss_step=77.90]#015Epoch 0:  28%|██▊       | 11/39 [00:02<00:07,  3.89it/s, loss=125, train_loss_step=77.90]#015Epoch 0:  28%|██▊       | 11/39 [00:02<00:07,  3.89it/s, loss=138, train_loss_step=269.0]#015Epoch 0:  31%|███       | 12/39 [00:03<00:06,  3.90it/s, loss=138, train_loss_step=269.0]#015Epoch 0:  31%|███       | 12/39 [00:03<00:06,  3.90it/s, loss=157, train_loss_step=368.0]#015Epoch 0:  33%|███▎      | 13/39 [00:03<00:06,  3.96it/s, loss=157, train_loss_step=368.0]#015Epoch 0:  33%|███▎      | 13/39 [00:03<00:06,  3.96it/s, loss=154, train_loss_step=109.0]#015Epoch 0:  36%|███▌      | 14/39 [00:03<00:06,  4.00it/s, loss=154, train_loss_step=109.0]#015Epoch 0:  36%|███▌      | 14/39 [00:03<00:06,  4.00it/s, loss=146, train_loss_step=50.10]#015Epoch 0:  38%|███▊      | 15/39 [00:03<00:06,  3.98it/s, loss=146, train_loss_step=50.10]#015Epoch 0:  38%|███▊      | 15/39 [00:03<00:06,  3.98it/s, loss=145, train_loss_step=122.0]#015Epoch 0:  41%|████      | 16/39 [00:04<00:05,  3.95it/s, loss=145, train_loss_step=122.0]#015Epoch 0:  41%|████      | 16/39 [00:04<00:05,  3.95it/s, loss=137, train_loss_step=17.00]#015Epoch 0:  44%|████▎     | 17/39 [00:04<00:05,  3.96it/s, loss=137, train_loss_step=17.00]#015Epoch 0:  44%|████▎     | 17/39 [00:04<00:05,  3.96it/s, loss=147, train_loss_step=320.0]#015Epoch 0:  46%|████▌     | 18/39 [00:04<00:05,  3.98it/s, loss=147, train_loss_step=320.0]#015Epoch 0:  46%|████▌     | 18/39 [00:04<00:05,  3.98it/s, loss=155, train_loss_step=280.0]#015Epoch 0:  49%|████▊     | 19/39 [00:04<00:04,  4.02it/s, loss=155, train_loss_step=280.0]#015Epoch 0:  49%|████▊     | 19/39 [00:04<00:04,  4.02it/s, loss=148, train_loss_step=16.70]#015Epoch 0:  51%|█████▏    | 20/39 [00:04<00:04,  4.05it/s, loss=148, train_loss_step=16.70]#015Epoch 0:  51%|█████▏    | 20/39 [00:04<00:04,  4.05it/s, loss=147, train_loss_step=135.0]#015Epoch 0:  54%|█████▍    | 21/39 [00:05<00:04,  4.03it/s, loss=147, train_loss_step=135.0]#015Epoch 0:  54%|█████▍    | 21/39 [00:05<00:04,  4.03it/s, loss=160, train_loss_step=283.0]#015Epoch 0:  56%|█████▋    | 22/39 [00:05<00:04,  4.04it/s, loss=160, train_loss_step=283.0]#015Epoch 0:  56%|█████▋    | 22/39 [00:05<00:04,  4.04it/s, loss=151, train_loss_step=13.70]#015Epoch 0:  59%|█████▉    | 23/39 [00:05<00:03,  4.06it/s, loss=151, train_loss_step=13.70]#015Epoch 0:  59%|█████▉    | 23/39 [00:05<00:03,  4.06it/s, loss=141, train_loss_step=54.50]#015Epoch 0:  62%|██████▏   | 24/39 [00:05<00:03,  4.05it/s, loss=141, train_loss_step=54.50]#015Epoch 0:  62%|██████▏   | 24/39 [00:05<00:03,  4.05it/s, loss=141, train_loss_step=10.80]#015Epoch 0:  64%|██████▍   | 25/39 [00:06<00:03,  4.06it/s, loss=141, train_loss_step=10.80]#015Epoch 0:  64%|██████▍   | 25/39 [00:06<00:03,  4.05it/s, loss=140, train_loss_step=344.0]#015Epoch 0:  67%|██████▋   | 26/39 [00:06<00:03,  4.07it/s, loss=140, train_loss_step=344.0]#015Epoch 0:  67%|██████▋   | 26/39 [00:06<00:03,  4.07it/s, loss=139, train_loss_step=49.80]#015Epoch 0:  69%|██████▉   | 27/39 [00:06<00:02,  4.08it/s, loss=139, train_loss_step=49.80]#015Epoch 0:  69%|██████▉   | 27/39 [00:06<00:02,  4.08it/s, loss=144, train_loss_step=275.0]#015Epoch 0:  72%|███████▏  | 28/39 [00:06<00:02,  4.08it/s, loss=144, train_loss_step=275.0]#015Epoch 0:  72%|███████▏  | 28/39 [00:06<00:02,  4.08it/s, loss=144, train_loss_step=65.30]#015Epoch 0:  74%|███████▍  | 29/39 [00:07<00:02,  4.10it/s, loss=144, train_loss_step=65.30]#015Epoch 0:  74%|███████▍  | 29/39 [00:07<00:02,  4.10it/s, loss=150, train_loss_step=148.0]#015Epoch 0:  77%|███████▋  | 30/39 [00:07<00:02,  4.08it/s, loss=150, train_loss_step=148.0]#015Epoch 0:  77%|███████▋  | 30/39 [00:07<00:02,  4.08it/s, loss=152, train_loss_step=108.0]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  11%|█         | 1/9 [00:00<00:03,  2.60it/s]#033[A#015Epoch 0:  82%|████████▏ | 32/39 [00:07<00:01,  4.12it/s, loss=152, train_loss_step=108.0]\u001b[0m\n",
      "\u001b[34m#015Validating:  33%|███▎      | 3/9 [00:00<00:01,  3.50it/s]#033[A#015Epoch 0:  87%|████████▋ | 34/39 [00:07<00:01,  4.31it/s, loss=152, train_loss_step=108.0]\u001b[0m\n",
      "\u001b[34m#015Validating:  56%|█████▌    | 5/9 [00:00<00:00,  4.60it/s]#033[A#015Epoch 0:  92%|█████████▏| 36/39 [00:08<00:00,  4.50it/s, loss=152, train_loss_step=108.0]\u001b[0m\n",
      "\u001b[34m#015Validating:  78%|███████▊  | 7/9 [00:00<00:00,  5.95it/s]#033[A#015Epoch 0:  97%|█████████▋| 38/39 [00:08<00:00,  4.69it/s, loss=152, train_loss_step=108.0]\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 9/9 [00:00<00:00,  7.14it/s]#033[A#015Epoch 0: 100%|██████████| 39/39 [00:08<00:00,  4.72it/s, loss=152, train_loss_step=108.0, val_loss=188.0]\u001b[0m\n",
      "\u001b[34m#015                                                         #033[A#015Epoch 0: 100%|██████████| 39/39 [00:08<00:00,  4.72it/s, loss=152, train_loss_step=108.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 0:   0%|          | 0/39 [00:00<?, ?it/s, loss=152, train_loss_step=108.0, val_loss=188.0, train_loss_epoch=143.0]         #015Epoch 1:   0%|          | 0/39 [00:00<?, ?it/s, loss=152, train_loss_step=108.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:   3%|▎         | 1/39 [00:00<00:17,  2.19it/s, loss=139, train_loss_step=9.280, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:   5%|▌         | 2/39 [00:00<00:13,  2.76it/s, loss=139, train_loss_step=9.280, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:   5%|▌         | 2/39 [00:00<00:13,  2.76it/s, loss=123, train_loss_step=50.50, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:   8%|▊         | 3/39 [00:00<00:11,  3.04it/s, loss=138, train_loss_step=407.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  10%|█         | 4/39 [00:01<00:10,  3.22it/s, loss=138, train_loss_step=407.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  10%|█         | 4/39 [00:01<00:10,  3.22it/s, loss=144, train_loss_step=170.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  13%|█▎        | 5/39 [00:01<00:10,  3.25it/s, loss=139, train_loss_step=19.70, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  15%|█▌        | 6/39 [00:01<00:10,  3.19it/s, loss=139, train_loss_step=19.70, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  15%|█▌        | 6/39 [00:01<00:10,  3.19it/s, loss=144, train_loss_step=116.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  18%|█▊        | 7/39 [00:02<00:09,  3.24it/s, loss=136, train_loss_step=171.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  21%|██        | 8/39 [00:02<00:09,  3.24it/s, loss=136, train_loss_step=171.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  21%|██        | 8/39 [00:02<00:09,  3.24it/s, loss=127, train_loss_step=86.10, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  23%|██▎       | 9/39 [00:02<00:09,  3.27it/s, loss=126, train_loss_step=2.060, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  26%|██▌       | 10/39 [00:02<00:08,  3.37it/s, loss=126, train_loss_step=2.060, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  26%|██▌       | 10/39 [00:02<00:08,  3.37it/s, loss=137, train_loss_step=354.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  28%|██▊       | 11/39 [00:03<00:08,  3.46it/s, loss=127, train_loss_step=93.50, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  31%|███       | 12/39 [00:03<00:07,  3.49it/s, loss=127, train_loss_step=93.50, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  31%|███       | 12/39 [00:03<00:07,  3.49it/s, loss=134, train_loss_step=142.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  33%|███▎      | 13/39 [00:03<00:07,  3.57it/s, loss=133, train_loss_step=42.10, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  36%|███▌      | 14/39 [00:03<00:06,  3.62it/s, loss=133, train_loss_step=42.10, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  36%|███▌      | 14/39 [00:03<00:06,  3.62it/s, loss=144, train_loss_step=223.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  38%|███▊      | 15/39 [00:04<00:06,  3.64it/s, loss=139, train_loss_step=237.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  41%|████      | 16/39 [00:04<00:06,  3.61it/s, loss=139, train_loss_step=237.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  41%|████      | 16/39 [00:04<00:06,  3.60it/s, loss=141, train_loss_step=89.70, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  44%|████▎     | 17/39 [00:04<00:06,  3.62it/s, loss=132, train_loss_step=113.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  46%|████▌     | 18/39 [00:04<00:05,  3.65it/s, loss=132, train_loss_step=113.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  46%|████▌     | 18/39 [00:04<00:05,  3.65it/s, loss=138, train_loss_step=179.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  49%|████▊     | 19/39 [00:05<00:05,  3.68it/s, loss=133, train_loss_step=42.60, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  51%|█████▏    | 20/39 [00:05<00:05,  3.70it/s, loss=133, train_loss_step=42.60, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  51%|█████▏    | 20/39 [00:05<00:05,  3.70it/s, loss=128, train_loss_step=18.90, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  54%|█████▍    | 21/39 [00:05<00:04,  3.69it/s, loss=128, train_loss_step=7.750, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  56%|█████▋    | 22/39 [00:05<00:04,  3.71it/s, loss=128, train_loss_step=7.750, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  56%|█████▋    | 22/39 [00:05<00:04,  3.71it/s, loss=133, train_loss_step=152.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  59%|█████▉    | 23/39 [00:06<00:04,  3.70it/s, loss=115, train_loss_step=38.40, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  62%|██████▏   | 24/39 [00:06<00:04,  3.69it/s, loss=115, train_loss_step=38.40, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  62%|██████▏   | 24/39 [00:06<00:04,  3.69it/s, loss=112, train_loss_step=105.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  64%|██████▍   | 25/39 [00:06<00:03,  3.70it/s, loss=113, train_loss_step=56.50, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  67%|██████▋   | 26/39 [00:06<00:03,  3.73it/s, loss=113, train_loss_step=56.50, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  67%|██████▋   | 26/39 [00:06<00:03,  3.73it/s, loss=122, train_loss_step=294.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  69%|██████▉   | 27/39 [00:07<00:03,  3.76it/s, loss=114, train_loss_step=5.580, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  72%|███████▏  | 28/39 [00:07<00:02,  3.77it/s, loss=114, train_loss_step=5.580, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  72%|███████▏  | 28/39 [00:07<00:02,  3.77it/s, loss=117, train_loss_step=152.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  74%|███████▍  | 29/39 [00:07<00:02,  3.77it/s, loss=119, train_loss_step=32.60, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  77%|███████▋  | 30/39 [00:07<00:02,  3.76it/s, loss=119, train_loss_step=32.60, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1:  77%|███████▋  | 30/39 [00:07<00:02,  3.76it/s, loss=107, train_loss_step=110.0, val_loss=188.0, train_loss_epoch=143.0]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  11%|█         | 1/9 [00:00<00:03,  2.21it/s]#033[A#015Epoch 1:  82%|████████▏ | 32/39 [00:08<00:01,  3.78it/s, loss=107, train_loss_step=110.0, val_loss=188.0, train_loss_epoch=143.0]\u001b[0m\n",
      "\u001b[34m#015Validating:  33%|███▎      | 3/9 [00:00<00:02,  2.97it/s]#033[A#015Epoch 1:  87%|████████▋ | 34/39 [00:08<00:01,  3.96it/s, loss=107, train_loss_step=110.0, val_loss=188.0, train_loss_epoch=143.0]\u001b[0m\n",
      "\u001b[34m#015Validating:  56%|█████▌    | 5/9 [00:00<00:01,  3.94it/s]#033[A#015Epoch 1:  92%|█████████▏| 36/39 [00:08<00:00,  4.13it/s, loss=107, train_loss_step=110.0, val_loss=188.0, train_loss_epoch=143.0]\u001b[0m\n",
      "\u001b[34m#015Validating:  89%|████████▉ | 8/9 [00:00<00:00,  5.17it/s]#033[A#015Epoch 1: 100%|██████████| 39/39 [00:08<00:00,  4.39it/s, loss=107, train_loss_step=110.0, val_loss=188.0, train_loss_epoch=143.0]#015Epoch 1: 100%|██████████| 39/39 [00:09<00:00,  4.33it/s, loss=107, train_loss_step=110.0, val_loss=142.0, train_loss_epoch=143.0]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-09-03 09:05:20 Uploading - Uploading generated training model\u001b[34m#015                                                         #033[A#015Epoch 1: 100%|██████████| 39/39 [00:09<00:00,  4.33it/s, loss=107, train_loss_step=110.0, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 1:   0%|          | 0/39 [00:00<?, ?it/s, loss=107, train_loss_step=110.0, val_loss=142.0, train_loss_epoch=117.0]         #015Epoch 2:   0%|          | 0/39 [00:00<?, ?it/s, loss=107, train_loss_step=110.0, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:   3%|▎         | 1/39 [00:00<00:22,  1.71it/s, loss=120, train_loss_step=369.0, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:   5%|▌         | 2/39 [00:00<00:16,  2.26it/s, loss=117, train_loss_step=73.60, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:   8%|▊         | 3/39 [00:01<00:14,  2.55it/s, loss=117, train_loss_step=73.60, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:   8%|▊         | 3/39 [00:01<00:14,  2.54it/s, loss=119, train_loss_step=84.60, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  10%|█         | 4/39 [00:01<00:12,  2.71it/s, loss=109, train_loss_step=25.80, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  13%|█▎        | 5/39 [00:01<00:11,  2.93it/s, loss=100, train_loss_step=59.40, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  15%|█▌        | 6/39 [00:01<00:10,  3.01it/s, loss=100, train_loss_step=59.40, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  15%|█▌        | 6/39 [00:01<00:10,  3.01it/s, loss=107, train_loss_step=228.0, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  18%|█▊        | 7/39 [00:02<00:10,  3.14it/s, loss=102, train_loss_step=11.00, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  21%|██        | 8/39 [00:02<00:09,  3.23it/s, loss=94, train_loss_step=15.00, val_loss=142.0, train_loss_epoch=117.0] #015Epoch 2:  23%|██▎       | 9/39 [00:02<00:09,  3.31it/s, loss=94, train_loss_step=15.00, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  23%|██▎       | 9/39 [00:02<00:09,  3.31it/s, loss=93.6, train_loss_step=33.50, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  26%|██▌       | 10/39 [00:02<00:08,  3.39it/s, loss=106, train_loss_step=260.0, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  28%|██▊       | 11/39 [00:03<00:08,  3.40it/s, loss=105, train_loss_step=3.040, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  31%|███       | 12/39 [00:03<00:07,  3.41it/s, loss=105, train_loss_step=3.040, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  31%|███       | 12/39 [00:03<00:07,  3.41it/s, loss=99.8, train_loss_step=39.70, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  33%|███▎      | 13/39 [00:03<00:07,  3.49it/s, loss=104, train_loss_step=130.0, val_loss=142.0, train_loss_epoch=117.0] #015Epoch 2:  36%|███▌      | 14/39 [00:03<00:07,  3.53it/s, loss=103, train_loss_step=70.20, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  38%|███▊      | 15/39 [00:04<00:06,  3.54it/s, loss=103, train_loss_step=70.20, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  38%|███▊      | 15/39 [00:04<00:06,  3.54it/s, loss=102, train_loss_step=34.50, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  41%|████      | 16/39 [00:04<00:06,  3.59it/s, loss=90.4, train_loss_step=70.40, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  44%|████▎     | 17/39 [00:04<00:06,  3.63it/s, loss=93.8, train_loss_step=73.50, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  46%|████▌     | 18/39 [00:04<00:05,  3.63it/s, loss=93.8, train_loss_step=73.50, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  46%|████▌     | 18/39 [00:04<00:05,  3.63it/s, loss=89.8, train_loss_step=71.30, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  49%|████▊     | 19/39 [00:05<00:05,  3.68it/s, loss=90.4, train_loss_step=45.00, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  51%|█████▏    | 20/39 [00:05<00:05,  3.72it/s, loss=88.6, train_loss_step=74.80, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  54%|█████▍    | 21/39 [00:05<00:04,  3.76it/s, loss=88.6, train_loss_step=74.80, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  54%|█████▍    | 21/39 [00:05<00:04,  3.76it/s, loss=70.4, train_loss_step=4.470, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  56%|█████▋    | 22/39 [00:05<00:04,  3.77it/s, loss=70.8, train_loss_step=81.50, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  59%|█████▉    | 23/39 [00:06<00:04,  3.79it/s, loss=71.3, train_loss_step=94.40, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  62%|██████▏   | 24/39 [00:06<00:03,  3.79it/s, loss=71.3, train_loss_step=94.40, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  62%|██████▏   | 24/39 [00:06<00:03,  3.79it/s, loss=77, train_loss_step=140.0, val_loss=142.0, train_loss_epoch=117.0]  #015Epoch 2:  64%|██████▍   | 25/39 [00:06<00:03,  3.78it/s, loss=75.7, train_loss_step=32.80, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  67%|██████▋   | 26/39 [00:06<00:03,  3.81it/s, loss=64.5, train_loss_step=5.240, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  69%|██████▉   | 27/39 [00:07<00:03,  3.84it/s, loss=64.5, train_loss_step=5.240, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  69%|██████▉   | 27/39 [00:07<00:03,  3.84it/s, loss=66.5, train_loss_step=51.40, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  72%|███████▏  | 28/39 [00:07<00:02,  3.87it/s, loss=70.8, train_loss_step=101.0, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  74%|███████▍  | 29/39 [00:07<00:02,  3.87it/s, loss=74.4, train_loss_step=104.0, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  77%|███████▋  | 30/39 [00:07<00:02,  3.87it/s, loss=74.4, train_loss_step=104.0, val_loss=142.0, train_loss_epoch=117.0]#015Epoch 2:  77%|███████▋  | 30/39 [00:07<00:02,  3.87it/s, loss=66, train_loss_step=93.20, val_loss=142.0, train_loss_epoch=117.0]  \u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  11%|█         | 1/9 [00:00<00:02,  2.69it/s]#033[A#015Epoch 2:  85%|████████▍ | 33/39 [00:08<00:01,  4.02it/s, loss=66, train_loss_step=93.20, val_loss=142.0, train_loss_epoch=117.0]\u001b[0m\n",
      "\u001b[34m#015Validating:  33%|███▎      | 3/9 [00:00<00:01,  3.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  56%|█████▌    | 5/9 [00:00<00:00,  4.60it/s]#033[A#015Epoch 2:  92%|█████████▏| 36/39 [00:08<00:00,  4.28it/s, loss=66, train_loss_step=93.20, val_loss=142.0, train_loss_epoch=117.0]\u001b[0m\n",
      "\u001b[34m#015Validating:  78%|███████▊  | 7/9 [00:00<00:00,  5.75it/s]#033[A#015Epoch 2: 100%|██████████| 39/39 [00:08<00:00,  4.53it/s, loss=66, train_loss_step=93.20, val_loss=142.0, train_loss_epoch=117.0]\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 9/9 [00:00<00:00,  7.01it/s]#033[A#015Epoch 2: 100%|██████████| 39/39 [00:08<00:00,  4.48it/s, loss=66, train_loss_step=93.20, val_loss=114.0, train_loss_epoch=117.0]\u001b[0m\n",
      "\u001b[34m#015                                                         #033[A#015Epoch 2: 100%|██████████| 39/39 [00:08<00:00,  4.48it/s, loss=66, train_loss_step=93.20, val_loss=114.0, train_loss_epoch=82.70]#015Epoch 2: 100%|██████████| 39/39 [00:08<00:00,  4.48it/s, loss=66, train_loss_step=93.20, val_loss=114.0, train_loss_epoch=82.70]\u001b[0m\n",
      "\u001b[34mGlobal seed set to 42\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "   | Name                               | Type                            | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m0  | loss                               | QuantileLoss                    | 0     \u001b[0m\n",
      "\u001b[34m1  | logging_metrics                    | ModuleList                      | 0     \u001b[0m\n",
      "\u001b[34m2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \u001b[0m\n",
      "\u001b[34m3  | prescalers                         | ModuleDict                      | 5.1 K \u001b[0m\n",
      "\u001b[34m4  | static_variable_selection          | VariableSelectionNetwork        | 525 K \u001b[0m\n",
      "\u001b[34m5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.2 M \u001b[0m\n",
      "\u001b[34m6  | decoder_variable_selection         | VariableSelectionNetwork        | 420 K \u001b[0m\n",
      "\u001b[34m7  | static_context_variable_selection  | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m10 | static_context_enrichment          | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m11 | lstm_encoder                       | LSTM                            | 206 K \u001b[0m\n",
      "\u001b[34m12 | lstm_decoder                       | LSTM                            | 206 K \u001b[0m\n",
      "\u001b[34m13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 51.5 K\u001b[0m\n",
      "\u001b[34m14 | post_lstm_add_norm_encoder         | AddNorm                         | 320   \u001b[0m\n",
      "\u001b[34m15 | static_enrichment                  | GatedResidualNetwork            | 128 K \u001b[0m\n",
      "\u001b[34m16 | multihead_attn                     | InterpretableMultiHeadAttention | 64.4 K\u001b[0m\n",
      "\u001b[34m17 | post_attn_gate_norm                | GateAddNorm                     | 51.8 K\u001b[0m\n",
      "\u001b[34m18 | pos_wise_ff                        | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m19 | pre_output_gate_norm               | GateAddNorm                     | 51.8 K\u001b[0m\n",
      "\u001b[34m20 | output_layer                       | Linear                          | 1.1 K \u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m3.4 M     Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m3.4 M     Total params\u001b[0m\n",
      "\u001b[34m13.569    Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 40. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n",
      "\u001b[34mGlobal seed set to 42\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 30. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"TFT.py\", line 317, in <module>\u001b[0m\n",
      "\u001b[34m2022-09-03 09:05:15,425 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "    _train(parser.parse_args())\n",
      "  File \"TFT.py\", line 249, in _train\n",
      "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/saving.py\", line 134, in load_from_checkpoint\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.6 TFT.py --data-filename stallion_data.parquet --metadata-filename stallion_metadata.json --num-epochs 3 --run-mode test\"\u001b[0m\n",
      "\u001b[34mGlobal seed set to 42\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "   | Name                               | Type                            | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m0  | loss                               | QuantileLoss                    | 0     \u001b[0m\n",
      "\u001b[34m1  | logging_metrics                    | ModuleList                      | 0     \u001b[0m\n",
      "\u001b[34m2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \u001b[0m\n",
      "\u001b[34m3  | prescalers                         | ModuleDict                      | 5.1 K \u001b[0m\n",
      "\u001b[34m4  | static_variable_selection          | VariableSelectionNetwork        | 525 K \u001b[0m\n",
      "\u001b[34m5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.2 M \u001b[0m\n",
      "\u001b[34m6  | decoder_variable_selection         | VariableSelectionNetwork        | 420 K \u001b[0m\n",
      "\u001b[34m7  | static_context_variable_selection  | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m10 | static_context_enrichment          | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m11 | lstm_encoder                       | LSTM                            | 206 K \u001b[0m\n",
      "\u001b[34m12 | lstm_decoder                       | LSTM                            | 206 K \u001b[0m\n",
      "\u001b[34m13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 51.5 K\u001b[0m\n",
      "\u001b[34m14 | post_lstm_add_norm_encoder         | AddNorm                         | 320   \u001b[0m\n",
      "\u001b[34m15 | static_enrichment                  | GatedResidualNetwork            | 128 K \u001b[0m\n",
      "\u001b[34m16 | multihead_attn                     | InterpretableMultiHeadAttention | 64.4 K\u001b[0m\n",
      "\u001b[34m17 | post_attn_gate_norm                | GateAddNorm                     | 51.8 K\u001b[0m\n",
      "\u001b[34m18 | pos_wise_ff                        | GatedResidualNetwork            | 103 K \u001b[0m\n",
      "\u001b[34m19 | pre_output_gate_norm               | GateAddNorm                     | 51.8 K\u001b[0m\n",
      "\u001b[34m20 | output_layer                       | Linear                          | 1.1 K \u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m3.4 M     Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m3.4 M     Total params\u001b[0m\n",
      "\u001b[34m13.569    Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 40. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n",
      "\u001b[34mGlobal seed set to 42\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 30. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"TFT.py\", line 317, in <module>\n",
      "    _train(parser.parse_args())\n",
      "  File \"TFT.py\", line 249, in _train\n",
      "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/saving.py\", line 134, in load_from_checkpoint\n",
      "    checkpoint = pl_load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 37, in load\n",
      "    with fs.open(path_or_url, \"rb\") as f:\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fsspec/spec.py\", line 948, in open\n",
      "    **kwargs,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fsspec/implementations/local.py\", line 120, in _open\n",
      "    return LocalFileOpener(path, mode, fs=self, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fsspec/implementations/local.py\", line 202, in __init__\n",
      "    self._open()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fsspec/implementations/local.py\", line 207, in _open\n",
      "    self.f = open(self.path, mode=self.mode)\u001b[0m\n",
      "\u001b[34mIsADirectoryError: [Errno 21] Is a directory: '/opt/ml/code'\n",
      "    checkpoint = pl_load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 37, in load\n",
      "    with fs.open(path_or_url, \"rb\") as f:\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fsspec/spec.py\", line 948, in open\n",
      "    **kwargs,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fsspec/implementations/local.py\", line 120, in _open\n",
      "    return LocalFileOpener(path, mode, fs=self, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fsspec/implementations/local.py\", line 202, in __init__\n",
      "    self._open()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fsspec/implementations/local.py\", line 207, in _open\n",
      "    self.f = open(self.path, mode=self.mode)\u001b[0m\n",
      "\u001b[34mIsADirectoryError: [Errno 21] Is a directory: '/opt/ml/code'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-09-03 09:05:49 Failed - Training job failed\n",
      "ProfilerReport-1662195376: NoIssuesFound\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job tft-pytorch-spot-1-2022-09-03-08-56-15-855: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 TFT.py --data-filename stallion_data.parquet --metadata-filename stallion_metadata.json --num-epochs 3 --run-mode test\"\nGlobal seed set to 42\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n   | Name                               | Type                            | Params\n----------------------------------------------------------------------------------------\n0  | loss                               | QuantileLoss                    | 0     \n1  | logging_metrics                    | ModuleList                      | 0     \n2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n3  | prescalers                         | ModuleDict                      | 5.1 K \n4  | static_variable_selection          | V",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ee557f6e5c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m spot_estimator.fit(\n\u001b[1;32m     45\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'All'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2107\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3851\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3852\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3390\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3393\u001b[0m             )\n\u001b[1;32m   3394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job tft-pytorch-spot-1-2022-09-03-08-56-15-855: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 TFT.py --data-filename stallion_data.parquet --metadata-filename stallion_metadata.json --num-epochs 3 --run-mode test\"\nGlobal seed set to 42\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n   | Name                               | Type                            | Params\n----------------------------------------------------------------------------------------\n0  | loss                               | QuantileLoss                    | 0     \n1  | logging_metrics                    | ModuleList                      | 0     \n2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n3  | prescalers                         | ModuleDict                      | 5.1 K \n4  | static_variable_selection          | V"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "        'num-epochs': 3, \n",
    "        'data-filename': \"stallion_data.parquet\",\n",
    "        'metadata-filename': \"stallion_metadata.json\",\n",
    "        'run-mode': 'test'\n",
    "    }\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path='s3://sagemaker-us-east-1-551329315830/tensorboard',\n",
    "    container_local_output_path='/lightning_logs'\n",
    ")\n",
    "\n",
    "spot_estimator  = PyTorch(entry_point='TFT_docker/TFT.py',\n",
    "                            dependencies=['TFT_docker/requirements.txt'],\n",
    "                            role=role,\n",
    "                            framework_version='1.7.1',\n",
    "                            py_version='py3',\n",
    "                            instance_count=1,\n",
    "#                             instance_type='local',\n",
    "#                             instance_type='ml.p3.2xlarge',\n",
    "                            instance_type='ml.p2.xlarge',\n",
    "#                             instance_type='ml.p2.8xlarge',\n",
    "                            base_job_name='tft-pytorch-spot-1',\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            checkpoint_s3_uri=checkpoint_s3_path,\n",
    "                            debugger_hook_config=False,\n",
    "                            input_mode = 'FastFile',\n",
    "                            use_spot_instances=use_spot_instances,\n",
    "                            max_run=max_run,\n",
    "                            max_wait=max_wait,\n",
    "                            tensorboard_output_config=tensorboard_output_config,\n",
    "                            metric_definitions=[\n",
    "                                   {'Name': 'train:loss', 'Regex': 'train_loss_epoch=(.*?),'},\n",
    "                                   {'Name': 'trainstep:loss', 'Regex': 'train_loss_step=(.*?),'},\n",
    "                                   {'Name': 'validation:loss', 'Regex': 'val_loss=(.*?),'}\n",
    "                                ]\n",
    "                           )\n",
    "\n",
    "spot_estimator.fit(\n",
    "                inputs,\n",
    "                logs = 'All'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "import tarfile\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "\n",
    "def download_data_from_s3(s3_uri, sagemaker_session):\n",
    "    S3Downloader.download(\n",
    "                        s3_uri=s3_uri, \n",
    "                        local_path = \"./trained_model_artifact\",\n",
    "                        sagemaker_session=sagemaker_session\n",
    "                )\n",
    "    return \"./trained_model_artifact/model.tar.gz\"\n",
    "\n",
    "model_path = download_data_from_s3(spot_estimator.model_data, sagemaker_session)\n",
    "tar = tarfile.open(model_path, \"r:gz\")\n",
    "\n",
    "checkpointed_model = TemporalFusionTransformer.load_from_checkpoint(tar.extractfile(member=tar.getmember(name=\"model_trainer.ckpt\")))\n",
    "best_tft = checkpointed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e1b03",
   "metadata": {},
   "source": [
    "## Get Dataset again, for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "import torch\n",
    "\n",
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "training_cutoff = training_data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    training_data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, training_data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96863891",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = checkpointed_model.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
