{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d512c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'551329315830.dkr.ecr.us-east-1.amazonaws.com/pytorch-tft-container-test:latest'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client=boto3.client('sts')\n",
    "account=client.get_caller_identity()['Account']\n",
    "\n",
    "my_session=boto3.session.Session()\n",
    "region=my_session.region_name\n",
    "\n",
    "algorithm_name=\"pytorch-tft-container-test\"\n",
    "ecr_image='{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "\n",
    "ecr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b69ba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\r\n",
      "Configure a credential helper to remove this warning. See\r\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\r\n",
      "\r\n",
      "Login Succeeded\r\n"
     ]
    }
   ],
   "source": [
    "! aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b833ffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker version: 2.103.0\n",
      "Checkpointing Path: s3://sagemaker-us-east-1-551329315830/checkpoints/checkpoint-badc94da\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import uuid\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "print('SageMaker version: ' + sagemaker.__version__)\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-cnn-cifar10'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_path = 's3://{}/checkpoints/checkpoint-{}'.format(bucket, checkpoint_suffix)\n",
    "\n",
    "print('Checkpointing Path: {}'.format(checkpoint_s3_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0901d427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointing directory timeseries_data exists\n",
      "saved raw data to timeseries_data/stallion_data.parquet\n",
      "Checkpointing directory timeseries_data exists\n",
      "saved metadata to timeseries_data/stallion_metadata.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-551329315830/data/timeseries_data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_timeseries import download_process_and_return_raw_data, save_local_and_upload_s3, metadata_json_upload_s3\n",
    "import sagemaker\n",
    "import uuid\n",
    "\n",
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "special_days = [\n",
    "        \"easter_day\",\n",
    "        \"good_friday\",\n",
    "        \"new_year\",\n",
    "        \"christmas\",\n",
    "        \"labor_day\",\n",
    "        \"independence_day\",\n",
    "        \"revolution_day_memorial\",\n",
    "        \"regional_games\",\n",
    "        \"fifa_u_17_world_cup\",\n",
    "        \"football_gold_cup\",\n",
    "        \"beer_capital\",\n",
    "        \"music_fest\",\n",
    "    ]\n",
    "\n",
    "training_metadata = {}\n",
    "training_metadata['time_idx'] = \"time_idx\"\n",
    "training_metadata['target'] = \"volume\"\n",
    "training_metadata['group_ids'] = [\"agency\", \"sku\"]\n",
    "training_metadata['min_encoder_length'] = max_encoder_length // 2      # keep encoder length long (as it is in the validation set)\n",
    "training_metadata['max_encoder_length'] = max_encoder_length\n",
    "training_metadata['min_prediction_length'] = 1      \n",
    "training_metadata['max_prediction_length'] = max_prediction_length\n",
    "training_metadata['static_categoricals'] = [\"agency\", \"sku\"]\n",
    "training_metadata['static_reals'] = [\"avg_population_2017\", \"avg_yearly_household_income_2017\"]\n",
    "training_metadata['time_varying_known_categoricals'] = [\"special_days\", \"month\"]\n",
    "training_metadata['variable_groups'] = {\"special_days\": special_days}\n",
    "training_metadata['time_varying_known_reals'] = [\"time_idx\", \"price_regular\", \"discount_in_percent\"]\n",
    "training_metadata['time_varying_unknown_categoricals'] = []\n",
    "training_metadata['time_varying_unknown_reals'] = [\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ]\n",
    "training_metadata['target_normalizer'] = {\n",
    "                            \"normalized_groups\": [\"agency\", \"sku\"],\n",
    "                            \"normalization_transformation\": 'softplus'\n",
    "                        }\n",
    "training_metadata['add_relative_time_idx'] = True\n",
    "training_metadata['add_target_scales'] = True\n",
    "training_metadata['add_encoder_length'] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# obtain & upload training data\n",
    "training_data = download_process_and_return_raw_data()\n",
    "inputs = save_local_and_upload_s3(training_data, sagemaker_session, bucket, data_filename=\"stallion_data\")\n",
    "\n",
    "# upload metadata\n",
    "training_metadata['training_cutoff'] = int(training_data[\"time_idx\"].max() - max_prediction_length)\n",
    "metadata_json_upload_s3(training_metadata, sagemaker_session, bucket, metadata_filename=\"stallion_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a1946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot_instances = False\n",
    "max_run=6000      # in seconds, after this, job will be terminated\n",
    "max_wait = 10 * max_run if use_spot_instances else None\n",
    "local_image_name = 'pytorch-tft-container-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7ec87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-22 11:02:43 Starting - Starting the training job...ProfilerReport-1661166163: InProgress\n",
      "...\n",
      "2022-08-22 11:03:37 Starting - Preparing the instances for training......\n",
      "2022-08-22 11:04:43 Downloading - Downloading input data...\n",
      "2022-08-22 11:05:09 Training - Downloading the training image....................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-08-22 11:08:28,835 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-08-22 11:08:28,860 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-08-22 11:08:28,875 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-08-22 11:08:29,389 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting pytorch-forecasting\n",
      "  Downloading pytorch_forecasting-0.9.0-py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34mCollecting fastparquet\n",
      "  Downloading fastparquet-0.8.0.tar.gz (400 kB)\u001b[0m\n",
      "\n",
      "2022-08-22 11:08:38 Training - Training image download completed. Training in progress.\u001b[34mRequirement already satisfied: pyarrow in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (4.0.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning<2.0.0,>=1.2.4\n",
      "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<2.0.0,>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (1.5.4)\u001b[0m\n",
      "\u001b[34mCollecting statsmodels\n",
      "  Downloading statsmodels-0.12.2-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (3.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn<0.25,>=0.23 in /opt/conda/lib/python3.6/site-packages (from pytorch-forecasting->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[34mCollecting optuna<3.0.0,>=2.3.0\n",
      "  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting alembic\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\u001b[0m\n",
      "\u001b[34mCollecting colorlog\n",
      "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting cliff\n",
      "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (4.51.0)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.4.40-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas<2.0.0,>=1.1.0->pytorch-forecasting->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas<2.0.0,>=1.1.0->pytorch-forecasting->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas<2.0.0,>=1.1.0->pytorch-forecasting->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics>=0.4.1\n",
      "  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 4)) (0.35.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 4)) (3.17.1)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2.10)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn<0.25,>=0.23->pytorch-forecasting->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn<0.25,>=0.23->pytorch-forecasting->-r requirements.txt (line 1)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch<2.0,>=1.7->pytorch-forecasting->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.5.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.2.4->pytorch-forecasting->-r requirements.txt (line 1)) (21.2.0)\u001b[0m\n",
      "\u001b[34mCollecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (191 kB)\u001b[0m\n",
      "\u001b[34mCollecting idna-ssl>=1.0\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCollecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mCollecting Mako\n",
      "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34mCollecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34mCollecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34mCollecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mCollecting autopage>=0.4.0\n",
      "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-2.5.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (0.2.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.6/site-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch-forecasting->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pytorch-forecasting->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pytorch-forecasting->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pytorch-forecasting->-r requirements.txt (line 1)) (8.2.0)\u001b[0m\n",
      "\u001b[34mCollecting patsy>=0.5\n",
      "  Downloading patsy-0.5.2-py2.py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fastparquet, idna-ssl, pyperclip\n",
      "  Building wheel for fastparquet (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for fastparquet (setup.py): finished with status 'done'\n",
      "  Created wheel for fastparquet: filename=fastparquet-0.8.0-cp36-cp36m-linux_x86_64.whl size=1256945 sha256=08e5f45895d1e7817394b0c465d0b5bc4b1ee7c5a16b6d111a4a1177a0099315\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/6a/4f/0fd8e8bcbc4b5b751186e363b5b03975d8643eee2975eed2ca\n",
      "  Building wheel for idna-ssl (setup.py): started\n",
      "  Building wheel for idna-ssl (setup.py): finished with status 'done'\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=394acf92b9eb1a2a4c03ff2edb5d6f89a91456d61dfb100c16375f36a6ba29d3\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "  Building wheel for pyperclip (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11106 sha256=40139b1d85791fb78b9fe246248ae9429c387532736df3a43cd336ab5875fca5\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/95/e30a7f0b44cb90642de3469f211a3218f93f871789b4f4b46c\u001b[0m\n",
      "\u001b[34mSuccessfully built fastparquet idna-ssl pyperclip\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyasn1-modules, oauthlib, multidict, frozenlist, cachetools, yarl, requests-oauthlib, pyperclip, pbr, importlib-metadata, idna-ssl, google-auth, charset-normalizer, asynctest, async-timeout, aiosignal, tensorboard-plugin-wit, tensorboard-data-server, stevedore, sqlalchemy, setuptools, pyDeprecate, PrettyTable, markdown, Mako, importlib-resources, grpcio, google-auth-oauthlib, cmd2, autopage, aiohttp, absl-py, torchmetrics, tensorboard, patsy, colorlog, cmaes, cliff, alembic, statsmodels, pytorch-lightning, optuna, cramjam, pytorch-forecasting, fastparquet\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.0.1\n",
      "    Uninstalling importlib-metadata-4.0.1:\n",
      "      Successfully uninstalled importlib-metadata-4.0.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 49.6.0.post20210108\n",
      "    Uninstalling setuptools-49.6.0.post20210108:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled setuptools-49.6.0.post20210108\u001b[0m\n",
      "\u001b[34mSuccessfully installed Mako-1.1.6 PrettyTable-2.5.0 absl-py-1.2.0 aiohttp-3.8.1 aiosignal-1.2.0 alembic-1.7.7 async-timeout-4.0.2 asynctest-0.13.0 autopage-0.5.1 cachetools-4.2.4 charset-normalizer-2.1.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.6.0 cramjam-2.5.0 fastparquet-0.8.0 frozenlist-1.2.0 google-auth-2.11.0 google-auth-oauthlib-0.4.6 grpcio-1.47.0 idna-ssl-1.1.0 importlib-metadata-4.8.3 importlib-resources-5.4.0 markdown-3.3.7 multidict-5.2.0 oauthlib-3.2.0 optuna-2.10.1 patsy-0.5.2 pbr-5.10.0 pyDeprecate-0.3.1 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.9.0 pytorch-lightning-1.5.10 requests-oauthlib-1.3.1 setuptools-59.5.0 sqlalchemy-1.4.40 statsmodels-0.12.2 stevedore-3.5.0 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.8.2 yarl-1.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-08-22 11:09:17,554 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"data-filename\": \"stallion_data.parquet\",\n",
      "        \"epochs\": 2,\n",
      "        \"metadata-filename\": \"stallion_metadata.json\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tft-pytorch-spot-1-2022-08-22-11-02-43-246\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-08-22-11-02-43-246/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"TFT\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"TFT.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"data-filename\":\"stallion_data.parquet\",\"epochs\":2,\"metadata-filename\":\"stallion_metadata.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=TFT.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=TFT\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-08-22-11-02-43-246/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"data-filename\":\"stallion_data.parquet\",\"epochs\":2,\"metadata-filename\":\"stallion_metadata.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tft-pytorch-spot-1-2022-08-22-11-02-43-246\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-08-22-11-02-43-246/source/sourcedir.tar.gz\",\"module_name\":\"TFT\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"TFT.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--data-filename\",\"stallion_data.parquet\",\"--epochs\",\"2\",\"--metadata-filename\",\"stallion_metadata.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATA-FILENAME=stallion_data.parquet\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_METADATA-FILENAME=stallion_metadata.json\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 TFT.py --data-filename stallion_data.parquet --epochs 2 --metadata-filename stallion_metadata.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mCheckpointing directory /opt/ml/checkpoints exists\u001b[0m\n",
      "\u001b[34mDevice Type: cuda\u001b[0m\n",
      "\u001b[34mLoad Time Series dataset from S3\u001b[0m\n",
      "\u001b[34mcreating dataloader\u001b[0m\n",
      "\u001b[34mget GPU information\u001b[0m\n",
      "\u001b[34mGPU count: 1\u001b[0m\n",
      "\u001b[34mcreate model trainer\u001b[0m\n",
      "\u001b[34mcreate model from dataset\u001b[0m\n",
      "\u001b[34mNumber of parameters in network: 29.7k\u001b[0m\n",
      "\u001b[34mtraining model\u001b[0m\n",
      "\u001b[34m#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]#015Validation sanity check: 100%|██████████| 1/1 [00:03<00:00,  3.56s/it]#015                                                                      #015#015Training: 0it [00:00, ?it/s]#015Training:   0%|          | 0/31 [00:00<?, ?it/s]#015Epoch 0:   0%|          | 0/31 [00:00<?, ?it/s] #015Epoch 0:   3%|▎         | 1/31 [00:01<00:37,  1.26s/it]#015Epoch 0:   3%|▎         | 1/31 [00:01<00:37,  1.26s/it, loss=104, v_num=0, train_loss_step=104.0]#015Epoch 0:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, loss=104, v_num=0, train_loss_step=104.0]#015Epoch 0:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, loss=97.8, v_num=0, train_loss_step=92.10]#015Epoch 0:  10%|▉         | 3/31 [00:02<00:19,  1.47it/s, loss=97.8, v_num=0, train_loss_step=92.10]#015Epoch 0:  10%|▉         | 3/31 [00:02<00:19,  1.47it/s, loss=99, v_num=0, train_loss_step=101.0]  #015Epoch 0:  13%|█▎        | 4/31 [00:02<00:16,  1.64it/s, loss=99, v_num=0, train_loss_step=101.0]#015Epoch 0:  13%|█▎        | 4/31 [00:02<00:16,  1.63it/s, loss=96.8, v_num=0, train_loss_step=90.00]#015Epoch 0:  16%|█▌        | 5/31 [00:02<00:14,  1.76it/s, loss=96.8, v_num=0, train_loss_step=90.00]#015Epoch 0:  16%|█▌        | 5/31 [00:02<00:14,  1.76it/s, loss=92.8, v_num=0, train_loss_step=77.10]#015Epoch 0:  19%|█▉        | 6/31 [00:03<00:13,  1.83it/s, loss=92.8, v_num=0, train_loss_step=77.10]#015Epoch 0:  19%|█▉        | 6/31 [00:03<00:13,  1.83it/s, loss=94.3, v_num=0, train_loss_step=101.0]#015Epoch 0:  23%|██▎       | 7/31 [00:03<00:12,  1.88it/s, loss=94.3, v_num=0, train_loss_step=101.0]#015Epoch 0:  23%|██▎       | 7/31 [00:03<00:12,  1.88it/s, loss=97.6, v_num=0, train_loss_step=118.0]#015Epoch 0:  26%|██▌       | 8/31 [00:04<00:11,  1.94it/s, loss=97.6, v_num=0, train_loss_step=118.0]#015Epoch 0:  26%|██▌       | 8/31 [00:04<00:11,  1.94it/s, loss=96.4, v_num=0, train_loss_step=87.90]#015Epoch 0:  29%|██▉       | 9/31 [00:04<00:11,  1.99it/s, loss=96.4, v_num=0, train_loss_step=87.90]#015Epoch 0:  29%|██▉       | 9/31 [00:04<00:11,  1.99it/s, loss=95.4, v_num=0, train_loss_step=87.00]#015Epoch 0:  32%|███▏      | 10/31 [00:04<00:10,  2.03it/s, loss=95.4, v_num=0, train_loss_step=87.00]#015Epoch 0:  32%|███▏      | 10/31 [00:04<00:10,  2.03it/s, loss=96, v_num=0, train_loss_step=102.0]  #015Epoch 0:  35%|███▌      | 11/31 [00:05<00:10,  1.96it/s, loss=96, v_num=0, train_loss_step=102.0]#015Epoch 0:  35%|███▌      | 11/31 [00:05<00:10,  1.96it/s, loss=93.4, v_num=0, train_loss_step=66.90]#015Epoch 0:  39%|███▊      | 12/31 [00:06<00:09,  1.98it/s, loss=93.4, v_num=0, train_loss_step=66.90]#015Epoch 0:  39%|███▊      | 12/31 [00:06<00:09,  1.98it/s, loss=92.4, v_num=0, train_loss_step=81.80]#015Epoch 0:  42%|████▏     | 13/31 [00:06<00:08,  2.01it/s, loss=92.4, v_num=0, train_loss_step=81.80]#015Epoch 0:  42%|████▏     | 13/31 [00:06<00:08,  2.01it/s, loss=92.3, v_num=0, train_loss_step=91.60]#015Epoch 0:  45%|████▌     | 14/31 [00:06<00:08,  2.04it/s, loss=92.3, v_num=0, train_loss_step=91.60]#015Epoch 0:  45%|████▌     | 14/31 [00:06<00:08,  2.04it/s, loss=92.5, v_num=0, train_loss_step=94.70]#015Epoch 0:  48%|████▊     | 15/31 [00:07<00:07,  2.06it/s, loss=92.5, v_num=0, train_loss_step=94.70]#015Epoch 0:  48%|████▊     | 15/31 [00:07<00:07,  2.06it/s, loss=92.6, v_num=0, train_loss_step=94.00]#015Epoch 0:  52%|█████▏    | 16/31 [00:07<00:07,  2.08it/s, loss=92.6, v_num=0, train_loss_step=94.00]#015Epoch 0:  52%|█████▏    | 16/31 [00:07<00:07,  2.08it/s, loss=94, v_num=0, train_loss_step=116.0]  #015Epoch 0:  55%|█████▍    | 17/31 [00:08<00:06,  2.10it/s, loss=94, v_num=0, train_loss_step=116.0]#015Epoch 0:  55%|█████▍    | 17/31 [00:08<00:06,  2.10it/s, loss=93.8, v_num=0, train_loss_step=89.30]#015Epoch 0:  58%|█████▊    | 18/31 [00:08<00:06,  2.11it/s, loss=93.8, v_num=0, train_loss_step=89.30]#015Epoch 0:  58%|█████▊    | 18/31 [00:08<00:06,  2.10it/s, loss=91.8, v_num=0, train_loss_step=58.00]#015Epoch 0:  61%|██████▏   | 19/31 [00:09<00:05,  2.10it/s, loss=91.8, v_num=0, train_loss_step=58.00]#015Epoch 0:  61%|██████▏   | 19/31 [00:09<00:05,  2.10it/s, loss=92.7, v_num=0, train_loss_step=110.0]#015Epoch 0:  65%|██████▍   | 20/31 [00:09<00:05,  2.11it/s, loss=92.7, v_num=0, train_loss_step=110.0]#015Epoch 0:  65%|██████▍   | 20/31 [00:09<00:05,  2.11it/s, loss=92.1, v_num=0, train_loss_step=79.60]#015Epoch 0:  68%|██████▊   | 21/31 [00:10<00:04,  2.07it/s, loss=92.1, v_num=0, train_loss_step=79.60]#015Epoch 0:  68%|██████▊   | 21/31 [00:10<00:04,  2.07it/s, loss=90.1, v_num=0, train_loss_step=64.30]#015Epoch 0:  71%|███████   | 22/31 [00:10<00:04,  2.08it/s, loss=90.1, v_num=0, train_loss_step=64.30]#015Epoch 0:  71%|███████   | 22/31 [00:10<00:04,  2.08it/s, loss=89, v_num=0, train_loss_step=70.50]  #015Epoch 0:  74%|███████▍  | 23/31 [00:10<00:03,  2.10it/s, loss=89, v_num=0, train_loss_step=70.50]#015Epoch 0:  74%|███████▍  | 23/31 [00:10<00:03,  2.10it/s, loss=87.7, v_num=0, train_loss_step=75.70]#015Epoch 0:  77%|███████▋  | 24/31 [00:11<00:03,  2.10it/s, loss=87.7, v_num=0, train_loss_step=75.70]#015Epoch 0:  77%|███████▋  | 24/31 [00:11<00:03,  2.10it/s, loss=86.9, v_num=0, train_loss_step=73.60]#015Epoch 0:  81%|████████  | 25/31 [00:11<00:02,  2.12it/s, loss=86.9, v_num=0, train_loss_step=73.60]#015Epoch 0:  81%|████████  | 25/31 [00:11<00:02,  2.12it/s, loss=87, v_num=0, train_loss_step=78.90]  #015Epoch 0:  84%|████████▍ | 26/31 [00:12<00:02,  2.13it/s, loss=87, v_num=0, train_loss_step=78.90]#015Epoch 0:  84%|████████▍ | 26/31 [00:12<00:02,  2.13it/s, loss=86.5, v_num=0, train_loss_step=90.80]#015Epoch 0:  87%|████████▋ | 27/31 [00:12<00:01,  2.14it/s, loss=86.5, v_num=0, train_loss_step=90.80]#015Epoch 0:  87%|████████▋ | 27/31 [00:12<00:01,  2.14it/s, loss=83.4, v_num=0, train_loss_step=56.20]#015Epoch 0:  90%|█████████ | 28/31 [00:13<00:01,  2.15it/s, loss=83.4, v_num=0, train_loss_step=56.20]#015Epoch 0:  90%|█████████ | 28/31 [00:13<00:01,  2.15it/s, loss=82.9, v_num=0, train_loss_step=78.20]#015Epoch 0:  94%|█████████▎| 29/31 [00:13<00:00,  2.15it/s, loss=82.9, v_num=0, train_loss_step=78.20]#015Epoch 0:  94%|█████████▎| 29/31 [00:13<00:00,  2.15it/s, loss=82.6, v_num=0, train_loss_step=81.10]#015Epoch 0:  97%|█████████▋| 30/31 [00:13<00:00,  2.16it/s, loss=82.6, v_num=0, train_loss_step=81.10]#015Epoch 0:  97%|█████████▋| 30/31 [00:13<00:00,  2.16it/s, loss=81.3, v_num=0, train_loss_step=76.00]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]#033[A#015Epoch 0: 100%|██████████| 31/31 [00:15<00:00,  2.00it/s, loss=81.3, v_num=0, train_loss_step=76.00, val_loss=111.0]\u001b[0m\n",
      "\n",
      "2022-08-22 11:10:18 Uploading - Uploading generated training model\u001b[34m#015                                                         #033[A#015Epoch 0: 100%|██████████| 31/31 [00:16<00:00,  1.87it/s, loss=81.3, v_num=0, train_loss_step=76.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 0:   0%|          | 0/31 [00:00<?, ?it/s, loss=81.3, v_num=0, train_loss_step=76.00, val_loss=111.0, train_loss_epoch=86.20]         #015Epoch 1:   0%|          | 0/31 [00:00<?, ?it/s, loss=81.3, v_num=0, train_loss_step=76.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, loss=81.3, v_num=0, train_loss_step=76.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, loss=81.6, v_num=0, train_loss_step=72.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:   6%|▋         | 2/31 [00:01<00:18,  1.57it/s, loss=81.6, v_num=0, train_loss_step=72.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:   6%|▋         | 2/31 [00:01<00:18,  1.57it/s, loss=81.1, v_num=0, train_loss_step=72.30, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  10%|▉         | 3/31 [00:01<00:15,  1.79it/s, loss=81.1, v_num=0, train_loss_step=72.30, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  10%|▉         | 3/31 [00:01<00:15,  1.79it/s, loss=79.2, v_num=0, train_loss_step=52.80, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  13%|█▎        | 4/31 [00:02<00:14,  1.92it/s, loss=79.2, v_num=0, train_loss_step=52.80, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  13%|█▎        | 4/31 [00:02<00:14,  1.92it/s, loss=78.3, v_num=0, train_loss_step=76.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  16%|█▌        | 5/31 [00:02<00:12,  2.01it/s, loss=78.3, v_num=0, train_loss_step=76.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  16%|█▌        | 5/31 [00:02<00:12,  2.01it/s, loss=78.2, v_num=0, train_loss_step=93.40, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  19%|█▉        | 6/31 [00:02<00:12,  2.05it/s, loss=78.2, v_num=0, train_loss_step=93.40, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  19%|█▉        | 6/31 [00:02<00:12,  2.05it/s, loss=77, v_num=0, train_loss_step=90.70, val_loss=111.0, train_loss_epoch=86.20]  #015Epoch 1:  23%|██▎       | 7/31 [00:03<00:11,  2.10it/s, loss=77, v_num=0, train_loss_step=90.70, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  23%|██▎       | 7/31 [00:03<00:11,  2.10it/s, loss=75.8, v_num=0, train_loss_step=65.10, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  26%|██▌       | 8/31 [00:03<00:10,  2.14it/s, loss=75.8, v_num=0, train_loss_step=65.10, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  26%|██▌       | 8/31 [00:03<00:10,  2.14it/s, loss=75.1, v_num=0, train_loss_step=44.20, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  29%|██▉       | 9/31 [00:04<00:10,  2.17it/s, loss=75.1, v_num=0, train_loss_step=44.20, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  29%|██▉       | 9/31 [00:04<00:10,  2.17it/s, loss=72.6, v_num=0, train_loss_step=59.50, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  32%|███▏      | 10/31 [00:04<00:09,  2.19it/s, loss=72.6, v_num=0, train_loss_step=59.50, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  32%|███▏      | 10/31 [00:04<00:09,  2.19it/s, loss=72.3, v_num=0, train_loss_step=74.40, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  35%|███▌      | 11/31 [00:05<00:09,  2.11it/s, loss=72.3, v_num=0, train_loss_step=74.40, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  35%|███▌      | 11/31 [00:05<00:09,  2.11it/s, loss=72.5, v_num=0, train_loss_step=66.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  39%|███▊      | 12/31 [00:05<00:09,  2.10it/s, loss=72.5, v_num=0, train_loss_step=66.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  39%|███▊      | 12/31 [00:05<00:09,  2.10it/s, loss=73, v_num=0, train_loss_step=81.10, val_loss=111.0, train_loss_epoch=86.20]  #015Epoch 1:  42%|████▏     | 13/31 [00:06<00:08,  2.11it/s, loss=73, v_num=0, train_loss_step=81.10, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  42%|████▏     | 13/31 [00:06<00:08,  2.11it/s, loss=73.1, v_num=0, train_loss_step=77.50, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  45%|████▌     | 14/31 [00:06<00:08,  2.12it/s, loss=73.1, v_num=0, train_loss_step=77.50, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  45%|████▌     | 14/31 [00:06<00:08,  2.12it/s, loss=73.9, v_num=0, train_loss_step=89.70, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  48%|████▊     | 15/31 [00:07<00:07,  2.14it/s, loss=73.9, v_num=0, train_loss_step=89.70, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  48%|████▊     | 15/31 [00:07<00:07,  2.14it/s, loss=74.1, v_num=0, train_loss_step=84.10, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  52%|█████▏    | 16/31 [00:07<00:07,  2.14it/s, loss=74.1, v_num=0, train_loss_step=84.10, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  52%|█████▏    | 16/31 [00:07<00:07,  2.14it/s, loss=74.1, v_num=0, train_loss_step=90.20, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  55%|█████▍    | 17/31 [00:07<00:06,  2.15it/s, loss=74.1, v_num=0, train_loss_step=90.20, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  55%|█████▍    | 17/31 [00:07<00:06,  2.15it/s, loss=74.7, v_num=0, train_loss_step=68.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  58%|█████▊    | 18/31 [00:08<00:06,  2.15it/s, loss=74.7, v_num=0, train_loss_step=68.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  58%|█████▊    | 18/31 [00:08<00:06,  2.15it/s, loss=74.1, v_num=0, train_loss_step=65.60, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  61%|██████▏   | 19/31 [00:08<00:05,  2.15it/s, loss=74.1, v_num=0, train_loss_step=65.60, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  61%|██████▏   | 19/31 [00:08<00:05,  2.15it/s, loss=72.9, v_num=0, train_loss_step=57.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  65%|██████▍   | 20/31 [00:09<00:05,  2.14it/s, loss=72.9, v_num=0, train_loss_step=57.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  65%|██████▍   | 20/31 [00:09<00:05,  2.14it/s, loss=72.6, v_num=0, train_loss_step=70.60, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  68%|██████▊   | 21/31 [00:10<00:04,  2.08it/s, loss=72.6, v_num=0, train_loss_step=70.60, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  68%|██████▊   | 21/31 [00:10<00:04,  2.08it/s, loss=72.5, v_num=0, train_loss_step=70.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  71%|███████   | 22/31 [00:10<00:04,  2.09it/s, loss=72.5, v_num=0, train_loss_step=70.00, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  71%|███████   | 22/31 [00:10<00:04,  2.09it/s, loss=72.5, v_num=0, train_loss_step=73.40, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  74%|███████▍  | 23/31 [00:10<00:03,  2.11it/s, loss=72.5, v_num=0, train_loss_step=73.40, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  74%|███████▍  | 23/31 [00:10<00:03,  2.11it/s, loss=74.2, v_num=0, train_loss_step=85.80, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  77%|███████▋  | 24/31 [00:11<00:03,  2.12it/s, loss=74.2, v_num=0, train_loss_step=85.80, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  77%|███████▋  | 24/31 [00:11<00:03,  2.12it/s, loss=73.9, v_num=0, train_loss_step=70.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  81%|████████  | 25/31 [00:11<00:02,  2.13it/s, loss=73.9, v_num=0, train_loss_step=70.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  81%|████████  | 25/31 [00:11<00:02,  2.13it/s, loss=71.9, v_num=0, train_loss_step=52.70, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  84%|████████▍ | 26/31 [00:12<00:02,  2.14it/s, loss=71.9, v_num=0, train_loss_step=52.70, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  84%|████████▍ | 26/31 [00:12<00:02,  2.14it/s, loss=70.8, v_num=0, train_loss_step=69.20, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  87%|████████▋ | 27/31 [00:12<00:01,  2.15it/s, loss=70.8, v_num=0, train_loss_step=69.20, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  87%|████████▋ | 27/31 [00:12<00:01,  2.15it/s, loss=70.8, v_num=0, train_loss_step=64.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  90%|█████████ | 28/31 [00:12<00:01,  2.16it/s, loss=70.8, v_num=0, train_loss_step=64.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  90%|█████████ | 28/31 [00:12<00:01,  2.16it/s, loss=72.6, v_num=0, train_loss_step=79.80, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  94%|█████████▎| 29/31 [00:13<00:00,  2.16it/s, loss=72.6, v_num=0, train_loss_step=79.80, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  94%|█████████▎| 29/31 [00:13<00:00,  2.16it/s, loss=73.4, v_num=0, train_loss_step=76.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  97%|█████████▋| 30/31 [00:13<00:00,  2.17it/s, loss=73.4, v_num=0, train_loss_step=76.90, val_loss=111.0, train_loss_epoch=86.20]#015Epoch 1:  97%|█████████▋| 30/31 [00:13<00:00,  2.16it/s, loss=73.4, v_num=0, train_loss_step=73.70, val_loss=111.0, train_loss_epoch=86.20]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]#033[A#015Epoch 1: 100%|██████████| 31/31 [00:15<00:00,  2.00it/s, loss=73.4, v_num=0, train_loss_step=73.70, val_loss=104.0, train_loss_epoch=86.20]\u001b[0m\n",
      "\u001b[34m#015                                                         #033[A#015Epoch 1: 100%|██████████| 31/31 [00:16<00:00,  1.87it/s, loss=73.4, v_num=0, train_loss_step=73.70, val_loss=104.0, train_loss_epoch=72.30]#015Epoch 1: 100%|██████████| 31/31 [00:16<00:00,  1.85it/s, loss=73.4, v_num=0, train_loss_step=73.70, val_loss=104.0, train_loss_epoch=72.30]\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "   | Name                               | Type                            | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m0  | loss                               | QuantileLoss                    | 0     \u001b[0m\n",
      "\u001b[34m1  | logging_metrics                    | ModuleList                      | 0     \u001b[0m\n",
      "\u001b[34m2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \u001b[0m\n",
      "\u001b[34m3  | prescalers                         | ModuleDict                      | 256   \u001b[0m\n",
      "\u001b[34m4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \u001b[0m\n",
      "\u001b[34m5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \u001b[0m\n",
      "\u001b[34m6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \u001b[0m\n",
      "\u001b[34m7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m11 | lstm_encoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m12 | lstm_decoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \u001b[0m\n",
      "\u001b[34m14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \u001b[0m\n",
      "\u001b[34m2022-08-22 11:10:14,733 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \u001b[0m\n",
      "\u001b[34m16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \u001b[0m\n",
      "\u001b[34m17 | post_attn_gate_norm                | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m19 | pre_output_gate_norm               | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m20 | output_layer                       | Linear                          | 119   \u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m29.7 K    Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m29.7 K    Total params\u001b[0m\n",
      "\u001b[34m0.119     Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 350. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 128. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-08-22 11:10:39 Completed - Training job completed\n",
      "Training seconds: 356\n",
      "Billable seconds: 356\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "        'epochs': 2, \n",
    "        'data-filename': \"stallion_data.parquet\",\n",
    "        'metadata-filename': \"stallion_metadata.json\"\n",
    "    }\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path='s3://sagemaker-us-east-1-551329315830/tensorboard',\n",
    "    container_local_output_path='/lightning_logs'\n",
    ")\n",
    "\n",
    "spot_estimator  = PyTorch(entry_point='TFT_docker/TFT.py',\n",
    "                            dependencies=['TFT_docker/requirements.txt'],\n",
    "                            role=role,\n",
    "                            framework_version='1.7.1',\n",
    "                            py_version='py3',\n",
    "                            instance_count=1,\n",
    "#                             instance_type='local',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "#                             instance_type='ml.p2.xlarge',\n",
    "                            base_job_name='tft-pytorch-spot-1',\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            checkpoint_s3_uri=checkpoint_s3_path,\n",
    "                            debugger_hook_config=False,\n",
    "                            input_mode = 'FastFile',\n",
    "                            use_spot_instances=use_spot_instances,\n",
    "                            max_run=max_run,\n",
    "                            max_wait=max_wait,\n",
    "                            tensorboard_output_config=tensorboard_output_config\n",
    "                           )\n",
    "\n",
    "                .fit(\n",
    "                inputs,\n",
    "                logs = 'All'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2195343e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExFileObject name='./trained_model_artifact/model.tar.gz'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "import tarfile\n",
    "import joblib\n",
    "\n",
    "def download_data_from_s3(s3_uri, sagemaker_session):\n",
    "    S3Downloader.download(\n",
    "                        s3_uri=s3_uri, \n",
    "                        local_path = \"./trained_model_artifact\",\n",
    "                        sagemaker_session=sagemaker_session\n",
    "                )\n",
    "    return \"./trained_model_artifact/model.tar.gz\"\n",
    "\n",
    "model_path = download_data_from_s3(spot_estimator.model_data, sagemaker_session)\n",
    "tar = tarfile.open(model_path, \"r:gz\")\n",
    "\n",
    "# tar.extractfile(member=tar.getmember(name=\"model.pth\"))\n",
    "# model = joblib.load(tar.extractfile(member=tar.getmember(name=\"model.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d26feb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-08-22-11-02-43-246/output/model.tar.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7542cda",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6706a1129de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mTemporalFusionTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhparams_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/utilities/cloud_io.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# any sort of BytesIO or similiar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected 'r' or 'w' in mode but got {mode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_buffer_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0m_check_seekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mraise_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seek\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mraise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                 \u001b[0;34m+\u001b[0m \u001b[0;34m\" Please pre-load the data into a buffer like io.BytesIO and\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                                 + \" try to load from it instead.\")\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "import torch\n",
    "\n",
    "TemporalFusionTransformer.load_from_checkpoint(torch.load(tar.extractfile(member=tar.getmember(name=\"model.pth\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce818f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "model_artifact_s3_location = spot_estimator.model_data  # \"s3://<BUCKET>/<PREFIX>/model.tar.gz\"\n",
    "\n",
    "# Create PyTorchModel from saved model artifact\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_artifact_s3_location,\n",
    "    role=role,\n",
    "    framework_version=\"1.8.0\",\n",
    "    py_version=\"py3\",\n",
    "    entry_point=\"TFT_docker/TFT.py\",\n",
    ")\n",
    "\n",
    "# Create transformer from PyTorchModel object\n",
    "transformer = pytorch_model.transformer(instance_count=1, instance_type=\"ml.c5.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c190df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-551329315830/tft-pytorch-spot-1-2022-08-22-11-02-43-246/output/model.tar.gz'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_artifact_s3_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c4604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------"
     ]
    }
   ],
   "source": [
    "# # deploy the trained model\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "predictor = spot_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
