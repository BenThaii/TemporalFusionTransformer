{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8722d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'551329315830.dkr.ecr.us-east-1.amazonaws.com/pytorch-tft-container-test:latest'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client=boto3.client('sts')\n",
    "account=client.get_caller_identity()['Account']\n",
    "\n",
    "my_session=boto3.session.Session()\n",
    "region=my_session.region_name\n",
    "\n",
    "algorithm_name=\"pytorch-tft-container-test\"\n",
    "ecr_image='{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "\n",
    "ecr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2965ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointing directory timeseries_data exists\n",
      "saved raw data to timeseries_data/stallion_data.parquet\n"
     ]
    }
   ],
   "source": [
    "from utils_timeseries import download_process_and_return_raw_data, save_local_and_upload_s3, metadata_upload_s3\n",
    "import sagemaker\n",
    "import uuid\n",
    "\n",
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "special_days = [\n",
    "        \"easter_day\",\n",
    "        \"good_friday\",\n",
    "        \"new_year\",\n",
    "        \"christmas\",\n",
    "        \"labor_day\",\n",
    "        \"independence_day\",\n",
    "        \"revolution_day_memorial\",\n",
    "        \"regional_games\",\n",
    "        \"fifa_u_17_world_cup\",\n",
    "        \"football_gold_cup\",\n",
    "        \"beer_capital\",\n",
    "        \"music_fest\",\n",
    "    ]\n",
    "\n",
    "training_metadata = {}\n",
    "training_metadata['time_idx'] = \"time_idx\"\n",
    "training_metadata['target'] = \"volume\"\n",
    "training_metadata['group_ids'] = [\"agency\", \"sku\"]\n",
    "training_metadata['min_encoder_length'] = max_encoder_length // 2      # keep encoder length long (as it is in the validation set)\n",
    "training_metadata['max_encoder_length'] = max_encoder_length\n",
    "training_metadata['min_prediction_length'] = 1      \n",
    "training_metadata['max_prediction_length'] = max_prediction_length\n",
    "training_metadata['static_categoricals'] = [\"agency\", \"sku\"]\n",
    "training_metadata['static_reals'] = [\"avg_population_2017\", \"avg_yearly_household_income_2017\"]\n",
    "training_metadata['time_varying_known_categoricals'] = [\"special_days\", \"month\"]\n",
    "training_metadata['variable_groups'] = {\"special_days\": special_days}\n",
    "training_metadata['time_varying_known_reals'] = [\"time_idx\", \"price_regular\", \"discount_in_percent\"]\n",
    "training_metadata['time_varying_unknown_categoricals'] = []\n",
    "training_metadata['time_varying_unknown_reals'] = [\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ]\n",
    "training_metadata['target_normalizer'] = {\n",
    "                            \"normalized_groups\": [\"agency\", \"sku\"],\n",
    "                            \"normalization_transformation\": 'softplus'\n",
    "                        }\n",
    "training_metadata['add_relative_time_idx'] = True\n",
    "training_metadata['add_target_scales'] = True\n",
    "training_metadata['add_encoder_length'] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# obtain training data\n",
    "training_data = download_process_and_return_raw_data()\n",
    "\n",
    "#obtain meta data\n",
    "training_metadata['training_cutoff'] = training_data[\"time_idx\"].max() - max_prediction_length\n",
    "metadata_upload_s3(training_metadata, sagemaker_session, bucket, metadata_filename=\"stallion_metadata\")\n",
    "inputs = save_local_and_upload_s3(training_data, sagemaker_session, bucket, data_filename=\"stallion_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b221ef0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_cutoff = training_data[\"time_idx\"].max() - max_prediction_length\n",
    "training_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c993eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "data = training_data\n",
    "\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx= training_metadata['time_idx'],\n",
    "    target= training_metadata['target'],\n",
    "    group_ids= training_metadata['group_ids'],\n",
    "    min_encoder_length= training_metadata['min_encoder_length'],  \n",
    "    max_encoder_length= training_metadata['max_encoder_length'],\n",
    "    min_prediction_length=training_metadata['min_prediction_length'],\n",
    "    max_prediction_length=training_metadata['max_prediction_length'],\n",
    "    static_categoricals=training_metadata['static_categoricals'],\n",
    "    static_reals=training_metadata['static_reals'],\n",
    "    time_varying_known_categoricals=training_metadata['time_varying_known_categoricals'],\n",
    "    variable_groups= training_metadata['variable_groups'],  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals= training_metadata['time_varying_known_reals'],\n",
    "    time_varying_unknown_categoricals= training_metadata['time_varying_unknown_categoricals'],\n",
    "    time_varying_unknown_reals= training_metadata['time_varying_unknown_reals'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "            groups= training_metadata['target_normalizer']['normalized_groups'], \n",
    "            transformation= training_metadata['target_normalizer']['normalization_transformation']\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx= training_metadata['add_relative_time_idx'],\n",
    "    add_target_scales= training_metadata['add_target_scales'],\n",
    "    add_encoder_length= training_metadata['add_encoder_length'],\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.03,\n",
    "        hidden_size=16,\n",
    "        attention_head_size=1,\n",
    "        dropout=0.1,\n",
    "        hidden_continuous_size=8,\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d5bd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointing directory timeseries_data exists\n",
      "saved metadata to timeseries_data/stallion_metadata.json\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ef4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "estimator=Estimator(\n",
    "    image_uri=ecr_image,\n",
    "    role=get_execution_role(),\n",
    "    base_job_name=algorithm_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p2.xlarge',\n",
    "    input_mode = \"Pipe\"\n",
    ")\n",
    "\n",
    "# start training\n",
    "estimator.fit(inputs)\n",
    "\n",
    "# # deploy the trained model\n",
    "# predictor=estimator.deploy(1, instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c11f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
